{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 3 Final Release Version.ipynb","provenance":[{"file_id":"1M2TJD4kjBooKVmN9-7aSa-MyOy8bB1As","timestamp":1617332672150},{"file_id":"1Hh_rJwTpQHsg6nUySaHSwbyi9T5wRnt9","timestamp":1610986146854}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d7ece88333da4a3e9ed51578b34cdf4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_184fd0001f1145b780a84d9003086c22","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c1544475e585422ba155d155e945318d","IPY_MODEL_f23806f9493544ae9f28b112695bcf94"]}},"184fd0001f1145b780a84d9003086c22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1544475e585422ba155d155e945318d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_04b94c61afca442a9f13456d0fd6266f","_dom_classes":[],"description":"training: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":6,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7bd366b52f074a958f7f64a5e1997c35"}},"f23806f9493544ae9f28b112695bcf94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a7433cb5185b4e9097ae1e20efada18b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6/6 [10:08&lt;00:00, 101.48s/epoch]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e58a3b45a0fd47d89b54b02397d0e152"}},"04b94c61afca442a9f13456d0fd6266f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7bd366b52f074a958f7f64a5e1997c35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7433cb5185b4e9097ae1e20efada18b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e58a3b45a0fd47d89b54b02397d0e152":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f41b3485991144709127e1287787057f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ef0cbc8dc80469fac2d1108aef716d5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_31b19212cbbb417cb90891dca05a9e9c","IPY_MODEL_3155b8b5ab9b445192dd094ff6fd3d26"]}},"5ef0cbc8dc80469fac2d1108aef716d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"31b19212cbbb417cb90891dca05a9e9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6b5c643cd7a34026becac6475a588435","_dom_classes":[],"description":"epoch 1: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b6f31f619f874aa788212905ca961f35"}},"3155b8b5ab9b445192dd094ff6fd3d26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a762e92ccdb04986b60cc894f721faaf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [10:08&lt;00:00,  1.36batch/s, current_loss=0.519, mean_loss=0.0728]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75ebb209ce3d433e828516f1d1a99df2"}},"6b5c643cd7a34026becac6475a588435":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b6f31f619f874aa788212905ca961f35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a762e92ccdb04986b60cc894f721faaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75ebb209ce3d433e828516f1d1a99df2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce20f7c49ad94a75be7e4276f5f88897":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4677f3b970c94a0a825014c7e56c3b6c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_778b49546c644a2989e7da6d13beb6ee","IPY_MODEL_32f1c9b66af44fe3bf283269889dc3d8"]}},"4677f3b970c94a0a825014c7e56c3b6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"778b49546c644a2989e7da6d13beb6ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ebf57017cf8d4ff2b48c17b164608ff1","_dom_classes":[],"description":"epoch 2: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e65ecdd917504b229b69439ae16b6cee"}},"32f1c9b66af44fe3bf283269889dc3d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e8e0de3b426e47aa9a5bfe87a237e0af","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [09:14&lt;00:00,  1.50batch/s, current_loss=0.434, mean_loss=0.0617]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_667386ce5ab2420ba05d8eeef0f02993"}},"ebf57017cf8d4ff2b48c17b164608ff1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e65ecdd917504b229b69439ae16b6cee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8e0de3b426e47aa9a5bfe87a237e0af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"667386ce5ab2420ba05d8eeef0f02993":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d24f7487374d43eb95dd5c418e4534b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3218874f8197410eb01ea94c9488fd8c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b08d0a05d4bd44c79ceb6228fe035ea8","IPY_MODEL_aee644a9b163487892063275b0437d84"]}},"3218874f8197410eb01ea94c9488fd8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b08d0a05d4bd44c79ceb6228fe035ea8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_60be7bd7270e4895ae664e8e7fbec951","_dom_classes":[],"description":"epoch 3: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c44142ec5ef9460994c5562ad7c24fe3"}},"aee644a9b163487892063275b0437d84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ad8b24747a247a9ad2a92e7d6416231","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [08:19&lt;00:00,  1.66batch/s, current_loss=0.393, mean_loss=0.0579]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba6b8658500e4dcbb2cd2eda1ff767a2"}},"60be7bd7270e4895ae664e8e7fbec951":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c44142ec5ef9460994c5562ad7c24fe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ad8b24747a247a9ad2a92e7d6416231":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ba6b8658500e4dcbb2cd2eda1ff767a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d4a9114b5d7e4370aab82211bcb042a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6311cd554f394767af84dd38f8cd81e5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2c30048e0c46435ea4acab6cd7e464bb","IPY_MODEL_16cafd626d0b477db67f7fc120304091"]}},"6311cd554f394767af84dd38f8cd81e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c30048e0c46435ea4acab6cd7e464bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_87fa5ba4a79e457e9193cfce2a232d9d","_dom_classes":[],"description":"epoch 4: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ba94e2c032d4ac3a7e8682cae2a495e"}},"16cafd626d0b477db67f7fc120304091":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b3fe4a7898bf4349965ae20e6c0045ef","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [07:26&lt;00:00,  1.86batch/s, current_loss=0.402, mean_loss=0.0549]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e6f2539e28334a97ba15158c5ab6da55"}},"87fa5ba4a79e457e9193cfce2a232d9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6ba94e2c032d4ac3a7e8682cae2a495e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b3fe4a7898bf4349965ae20e6c0045ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e6f2539e28334a97ba15158c5ab6da55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fa5f7d13b6f478496e4c6ef05fc1f65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_91e9aea2f05d4280992f8a352a7f33eb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_21b15e9dfe5f4391afc1e914e7dc9e00","IPY_MODEL_6ea14f3f81e54dc0ab2f64898e2609bc"]}},"91e9aea2f05d4280992f8a352a7f33eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"21b15e9dfe5f4391afc1e914e7dc9e00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1f52aa54e12843c9a4a0960b1315b479","_dom_classes":[],"description":"epoch 5: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8728a8c48a0c4be88862d527e43c4cd3"}},"6ea14f3f81e54dc0ab2f64898e2609bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ca412840766d412b8c26c867fd41bd0e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [06:32&lt;00:00,  2.12batch/s, current_loss=0.281, mean_loss=0.0523]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef7058e3b7c24687addeaad6953135e4"}},"1f52aa54e12843c9a4a0960b1315b479":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8728a8c48a0c4be88862d527e43c4cd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ca412840766d412b8c26c867fd41bd0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef7058e3b7c24687addeaad6953135e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e1b737364094188a562939ab3b901de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_87d3bb8fcb7847ebbf78750063a1844e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_61d0147c63574248a02ce0fd7a7d26f6","IPY_MODEL_2a4225e4151b4147990ff81f7652ada8"]}},"87d3bb8fcb7847ebbf78750063a1844e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61d0147c63574248a02ce0fd7a7d26f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b2777706be844dc8a422017138f0bcd9","_dom_classes":[],"description":"epoch 6: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9d181849bf944a11bb549be126784cc7"}},"2a4225e4151b4147990ff81f7652ada8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8a34cf0e4ec4471aac8d23f995a8cf29","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [00:54&lt;00:00, 15.11batch/s, current_loss=0.427, mean_loss=0.05]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bc69f4e60b534fd4abcb035a99afa78f"}},"b2777706be844dc8a422017138f0bcd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9d181849bf944a11bb549be126784cc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a34cf0e4ec4471aac8d23f995a8cf29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bc69f4e60b534fd4abcb035a99afa78f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c937daa32b874bdab737e084d5f9debb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_222d404641274d9d8e61be4340478ebb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2ed3fd4cf0bd489581db2409a0e8c292","IPY_MODEL_bf86944d00f74f7c893013945815b808"]}},"222d404641274d9d8e61be4340478ebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ed3fd4cf0bd489581db2409a0e8c292":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_964baaed9d754828a50991c8c0fe94db","_dom_classes":[],"description":"training: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":8,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":8,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ddbd671f5964d9ebdc0b7fc711001dc"}},"bf86944d00f74f7c893013945815b808":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_24c6a88391574c649418f9b677608ce1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8/8 [17:54&lt;00:00, 134.30s/epoch]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_47113b9a46b941bbb15ac76014b35f6e"}},"964baaed9d754828a50991c8c0fe94db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0ddbd671f5964d9ebdc0b7fc711001dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"24c6a88391574c649418f9b677608ce1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"47113b9a46b941bbb15ac76014b35f6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a54b3d9a10d24712ad011b27b76bd2c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_62114473e8a54d9d98df8354b5cf2d4e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bc82f9c1a5474e71b78a2c06542d76ec","IPY_MODEL_bc1945b60c584fe7bb98fba3e127f8da"]}},"62114473e8a54d9d98df8354b5cf2d4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc82f9c1a5474e71b78a2c06542d76ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_def3c09022504988a8f6b50379e95b55","_dom_classes":[],"description":"epoch 1: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_12cad9c34c0f4a4d903ba05cfc998668"}},"bc1945b60c584fe7bb98fba3e127f8da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_29eb432a456648f8a5bd44d47a6e5ec9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [06:08&lt;00:00,  2.25batch/s, current_loss=0.413, mean_loss=0.0707]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_544d2e9877464e9d878fa1b1ce749997"}},"def3c09022504988a8f6b50379e95b55":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"12cad9c34c0f4a4d903ba05cfc998668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29eb432a456648f8a5bd44d47a6e5ec9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"544d2e9877464e9d878fa1b1ce749997":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f233312871d04888a2d5c355f0eaeeb7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c9a2838274224cb5ae7c70c87223be67","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4f400f45e6044c1dbfa8b509bed866e1","IPY_MODEL_99f0e65023934b058a5412bca39e1afc"]}},"c9a2838274224cb5ae7c70c87223be67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f400f45e6044c1dbfa8b509bed866e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_04b4985e628d49a5acf55765f9ef8c25","_dom_classes":[],"description":"epoch 2: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_87cdd241b3554df89a9b4ff3f01c4612"}},"99f0e65023934b058a5412bca39e1afc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_eae7e216c5744b73809046aa733954cb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [05:06&lt;00:00,  2.71batch/s, current_loss=0.539, mean_loss=0.0611]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0050c26bf6c3452d8ee90efb6b0ae9cb"}},"04b4985e628d49a5acf55765f9ef8c25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"87cdd241b3554df89a9b4ff3f01c4612":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eae7e216c5744b73809046aa733954cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0050c26bf6c3452d8ee90efb6b0ae9cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"879091214e344b7788df8dfd76f7c97c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b646a6e67ba94ee4847f7c2a2887f95e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b199b60c88a347358a542d994d381e3b","IPY_MODEL_696a4942fc994be59e35522b63231200"]}},"b646a6e67ba94ee4847f7c2a2887f95e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b199b60c88a347358a542d994d381e3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5d1fa3c94ff2492d8aa255eea25275a1","_dom_classes":[],"description":"epoch 3: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f813f24d16b4ba0903efe3beed0c3b6"}},"696a4942fc994be59e35522b63231200":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4068655b0505460580bfcabb716809d0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [04:04&lt;00:00,  3.39batch/s, current_loss=0.437, mean_loss=0.0577]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1f7e82ee9151418997ebc8515fbcc645"}},"5d1fa3c94ff2492d8aa255eea25275a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3f813f24d16b4ba0903efe3beed0c3b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4068655b0505460580bfcabb716809d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1f7e82ee9151418997ebc8515fbcc645":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ce1f8e648c94f8ba6ac9d274a8e6bfe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5cb611bf81124d7f84a0edcd0ffbe239","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_45386a10ac2a4698a323b6d776ed72f1","IPY_MODEL_94ae6ad1783d4e0f90fa4d0239357e09"]}},"5cb611bf81124d7f84a0edcd0ffbe239":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45386a10ac2a4698a323b6d776ed72f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a15df5aa2173465b8b73cea684234033","_dom_classes":[],"description":"epoch 4: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5f7dd1f66c3e427c929dbc0c55e200d2"}},"94ae6ad1783d4e0f90fa4d0239357e09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77f6b20085834b2789a9aa206406937f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [03:02&lt;00:00,  4.54batch/s, current_loss=0.298, mean_loss=0.0547]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a991035743864f22b679964d71db99b1"}},"a15df5aa2173465b8b73cea684234033":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5f7dd1f66c3e427c929dbc0c55e200d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77f6b20085834b2789a9aa206406937f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a991035743864f22b679964d71db99b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"23abef220fd54755bd2deb1402fc2d75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_702e82fd639c421193e00cb63614d697","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_49786989b84147c99f3b97f9561d61ee","IPY_MODEL_72237d5909b143b8b1bb17d141dc0257"]}},"702e82fd639c421193e00cb63614d697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49786989b84147c99f3b97f9561d61ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f82dcac3b48f4b50897fddfbc7d02001","_dom_classes":[],"description":"epoch 5: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0147a4be8ff4399a2dd96280f1ae596"}},"72237d5909b143b8b1bb17d141dc0257":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_430e701c88824b21ad8fa23640054286","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [02:01&lt;00:00,  6.81batch/s, current_loss=0.317, mean_loss=0.0521]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ffea3f205c2e4d7a80fdf853b3a4f25d"}},"f82dcac3b48f4b50897fddfbc7d02001":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b0147a4be8ff4399a2dd96280f1ae596":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"430e701c88824b21ad8fa23640054286":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ffea3f205c2e4d7a80fdf853b3a4f25d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db80c8115c4c47ec93716ffd0de8cd6c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_64a5f65e533e4cc2a8e77bb8ab5b89af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b7925ddd778740c9ac04c4f442a4e3f2","IPY_MODEL_4952657edf714eca85f40d9641a10e9d"]}},"64a5f65e533e4cc2a8e77bb8ab5b89af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7925ddd778740c9ac04c4f442a4e3f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6ba4147744d24502af2e6cbe7b278c0e","_dom_classes":[],"description":"epoch 6: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_67488eb083d047a7b751bd34ed6ee860"}},"4952657edf714eca85f40d9641a10e9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa3b3fbacf864b5791ca39ea24fd3210","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [12:46&lt;00:00,  1.08batch/s, current_loss=0.347, mean_loss=0.0495]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3dcf1b84234242dd8e3ecc01aea8a0bc"}},"6ba4147744d24502af2e6cbe7b278c0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"67488eb083d047a7b751bd34ed6ee860":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa3b3fbacf864b5791ca39ea24fd3210":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3dcf1b84234242dd8e3ecc01aea8a0bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"06bb1e329f074326a5ec0265a530abf9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3842299df5c2467096ec82b9978b520f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9d88b6257a6946e2aac99eeec8d635c8","IPY_MODEL_604d0736c1d14eba973ae8530eb83390"]}},"3842299df5c2467096ec82b9978b520f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9d88b6257a6946e2aac99eeec8d635c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d7cf2b9878ab449ba6be8676e065cfa0","_dom_classes":[],"description":"epoch 7: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c180a891a58347048bcd39e55d3a3b33"}},"604d0736c1d14eba973ae8530eb83390":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2d50a98d275c4f39a1aac87f882acab4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [11:46&lt;00:00,  1.18batch/s, current_loss=0.3, mean_loss=0.0469]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2590b8109e8e498e87510bb627dd435f"}},"d7cf2b9878ab449ba6be8676e065cfa0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c180a891a58347048bcd39e55d3a3b33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2d50a98d275c4f39a1aac87f882acab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2590b8109e8e498e87510bb627dd435f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"11c748da38c4461884733c03b9f143d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_20714c0314e040ba9e4ae42d30b94e10","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8823d9c4cfa34e158868e1a7fd95c0dc","IPY_MODEL_3f87df90a7eb45678a51a82433ea877d"]}},"20714c0314e040ba9e4ae42d30b94e10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8823d9c4cfa34e158868e1a7fd95c0dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7a260134c53c4fb08728f254561eb459","_dom_classes":[],"description":"epoch 8: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":830,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":830,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_edfc078456d44140bf38c91b16e136e1"}},"3f87df90a7eb45678a51a82433ea877d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b365c851f04641b08088bedbdf1f6a43","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 830/830 [01:01&lt;00:00, 13.50batch/s, current_loss=0.467, mean_loss=0.0445]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_25cb33b2d51149579f8a959f2be6f6d2"}},"7a260134c53c4fb08728f254561eb459":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"edfc078456d44140bf38c91b16e136e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b365c851f04641b08088bedbdf1f6a43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"25cb33b2d51149579f8a959f2be6f6d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"MA4r_0ZqcSfc"},"source":["# Licensing Information:  You are free to use or extend this project for\n","# educational purposes provided that (1) you do not distribute or publish\n","# solutions, (2) you retain this notice, and (3) you provide clear\n","# attribution to The Georgia Institute of Technology, including a link to https://aritter.github.io/CS-7650/\n","\n","# Attribution Information: \n","# This Project was developed at the Georgia Institute of Technology by Ashutosh Baheti (ashutosh.baheti@cc.gatech.edu), \n","# borrowing  from the Neural Machine Translation Project (Project 2) \n","# of the UC Berkeley NLP course https://cal-cs288.github.io/sp20/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxiBjiN7caiJ"},"source":["# Project #3: Neural Chatbot"]},{"cell_type":"markdown","metadata":{"id":"qCA9aMQxcjoc"},"source":["Neural Dialog Model are Sequence-to-Sequence (Seq2Seq) models that produce conversational response given the dialog history. State-of-the-art dialog models are trained on millions of multi-turn conversations. However, in this assignment we will narrow our scope to single turn conversations to make the problem easier.  \n","\n","In this assignment you will implement,\n","1. Seq2Seq encoder-decoder model\n","2. Seq2Seq model with attention mechanism\n","3. Greedy and Beam search decoding algorithms  "]},{"cell_type":"markdown","metadata":{"id":"D0xB9Vlheg6c"},"source":["First import libraries required for the implementation"]},{"cell_type":"code","metadata":{"id":"TIkNu1zhLAtR"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import unicode_literals\n","\n","import torch\n","from torch.jit import script, trace\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import numpy as np\n","import csv\n","import random\n","import re\n","import os\n","import unicodedata\n","import codecs\n","from io import open\n","import itertools\n","import math\n","import pickle\n","import statistics\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","import tqdm\n","import nltk\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3ny4wUTeqW2"},"source":["Then we implement some standard util functions that will be useful in the rest of the code."]},{"cell_type":"code","metadata":{"id":"SRA1Fu5vLWtB"},"source":["# General util functions\n","def make_dir_if_not_exists(directory):\n","\tif not os.path.exists(directory):\n","\t\tlogging.info(\"Creating new directory: {}\".format(directory))\n","\t\tos.makedirs(directory)\n","\n","def print_list(l, K=None):\n","\t# If K is given then only print first K\n","\tfor i, e in enumerate(l):\n","\t\tif i == K:\n","\t\t\tbreak\n","\t\tprint(e)\n","\tprint()\n","\n","def remove_multiple_spaces(string):\n","\treturn re.sub(r'\\s+', ' ', string).strip()\n","\n","def save_in_pickle(save_13.77object, save_file):\n","\twith open(save_file, \"wb\") as pickle_out:\n","\t\tpickle.dump(save_object, pickle_out)\n","\n","def load_from_pickle(pickle_file):\n","\twith open(pickle_file, \"rb\") as pickle_in:\n","\t\treturn pickle.load(pickle_in)\n","\n","def save_in_txt(list_of_strings, save_file):\n","\twith open(save_file, \"w\") as writer:\n","\t\tfor line in list_of_strings:\n","\t\t\tline = line.strip()\n","\t\t\twriter.write(f\"{line}\\n\")\n","\n","def load_from_txt(txt_file):\n","\twith open(txt_file, \"r\") as reader:\n","\t\tall_lines = list()\n","\t\tfor line in reader:\n","\t\t\tline = line.strip()\n","\t\t\tall_lines.append(line)\n","\t\treturn all_lines"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9Mi3azJe0I8"},"source":["Finally we will check if GPU is available and set the device accordingly.\n","\n","Tip: While debugging use `CPU` and change the runtime type to `GPU` when you are ready to train your models to efficiently use free Colab GPU"]},{"cell_type":"code","metadata":{"id":"VFOWkGQdNUUo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618943939066,"user_tz":240,"elapsed":600,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"4d59d2ce-2194-442c-f714-d06a19dd9f26"},"source":["print(torch.cuda.is_available())\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(\"Using device:\", device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","Using device: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pA4RhP1Rfegu"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"Sjw3wbo8fipA"},"source":["For the dataset we will be using a small sample of single turn input and response pairs from [Cornell Movie Dialog Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). We filter conversational pairs with sentences > 10 tokens. To reduce your work, we have already created a sample of tokenized, lowercased single turn conversations from Cornell Movie Dialog Corpus. The preprocessed dataset sample is stored in pickle format and can be downloaded from [this link](https://drive.google.com/file/d/1qYdSlDJ89AvgozK3V5tik8Op93zPbG6e/view?usp=sharing). Please download the `processed_CMDC.pkl` file from the link and upload it in colab."]},{"cell_type":"code","metadata":{"id":"9l4KOwIsNWqb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618943940793,"user_tz":240,"elapsed":487,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"a78c3935-7458-48c1-a821-c20b1ea74f46"},"source":["# Loading the pre-processed conversational exchanges (source-target pairs) from pickle data files\n","all_conversations = load_from_pickle(\"processed_CMDC.pkl\")\n","# Extract 100 conversations from the end for evaluation and keep the rest for training\n","eval_conversations = all_conversations[-100:]\n","all_conversations = all_conversations[:-100]\n","\n","# Logging data stats\n","print(f\"Number of Training Conversation Pairs = {len(all_conversations)}\")\n","print(f\"Number of Evaluation Conversation Pairs = {len(eval_conversations)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of Training Conversation Pairs = 53065\n","Number of Evaluation Conversation Pairs = 100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OHI2pdQDgoEB"},"source":["Let's print a couple of conversations to check if they are loaded properly."]},{"cell_type":"code","metadata":{"id":"MACN38BoN7OQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618943942738,"user_tz":240,"elapsed":610,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"8064375f-3825-4a59-871f-e405142735ad"},"source":["print_list(all_conversations, 5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('there .', 'where ?')\n","('you have my word . as a gentleman', 'you re sweet .')\n","('hi .', 'looks like things worked out tonight huh ?')\n","('have fun tonight ?', 'tons')\n","('well no . . .', 'then that s all you had to say .')\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QYKyOii3p0Hb"},"source":["## Vocabulary"]},{"cell_type":"markdown","metadata":{"id":"2Bqccrbvgugd"},"source":["The words in the sentences need to be converted into integer tokens so that the neural model can operate on them. For this purpose, we will create a vocabulary which will convert the input strings into model recognizable integer tokens."]},{"cell_type":"code","metadata":{"id":"3zfGojqcqmLK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618943945924,"user_tz":240,"elapsed":2019,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"8b77ff55-f44c-46c9-eaf4-511932186cfd"},"source":["pad_word = \"<pad>\"\n","bos_word = \"<s>\"\n","eos_word = \"</s>\"\n","unk_word = \"<unk>\"\n","pad_id = 0\n","bos_id = 1\n","eos_id = 2\n","unk_id = 3\n","    \n","def normalize_sentence(s):\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","class Vocabulary:\n","    def __init__(self):\n","        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id}\n","        self.word_count = {}\n","        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word}\n","        self.num_words = 4\n","    \n","    def get_ids_from_sentence(self, sentence):\n","        sentence = normalize_sentence(sentence)\n","        sent_ids = [bos_id] + [self.word_to_id[word] if word in self.word_to_id \\\n","                               else unk_id for word in sentence.split()] + \\\n","                               [eos_id]\n","        return sent_ids\n","    \n","    def tokenized_sentence(self, sentence):\n","        sent_ids = self.get_ids_from_sentence(sentence)\n","        return [self.id_to_word[word_id] for word_id in sent_ids]\n","\n","    def decode_sentence_from_ids(self, sent_ids):\n","        words = list()\n","        for i, word_id in enumerate(sent_ids):\n","            if word_id in [bos_id, eos_id, pad_id]:\n","                # Skip these words\n","                continue\n","            else:\n","                words.append(self.id_to_word[word_id])\n","        return ' '.join(words)\n","\n","    def add_words_from_sentence(self, sentence):\n","        sentence = normalize_sentence(sentence)\n","        for word in sentence.split():\n","            if word not in self.word_to_id:\n","                # add this word to the vocabulary\n","                self.word_to_id[word] = self.num_words\n","                self.id_to_word[self.num_words] = word\n","                self.word_count[word] = 1\n","                self.num_words += 1\n","            else:\n","                # update the word count\n","                self.word_count[word] += 1\n","\n","vocab = Vocabulary()\n","for src, tgt in all_conversations:\n","    vocab.add_words_from_sentence(src)\n","    vocab.add_words_from_sentence(tgt)\n","print(f\"Total words in the vocabulary = {vocab.num_words}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total words in the vocabulary = 7727\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LBouL7ZHyvxM"},"source":["Let's print top 30 vocab words:"]},{"cell_type":"code","metadata":{"id":"1dlNBVwc0dzI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618943947668,"user_tz":240,"elapsed":788,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"1ac2fe44-2c5d-46cc-fc37-3af1bd8e878f"},"source":["print_list(sorted(vocab.word_count.items(), key=lambda item: item[1], reverse=True), 30)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('.', 84255)\n","('?', 36822)\n","('you', 25093)\n","('i', 18946)\n","('what', 10765)\n","('s', 10089)\n","('it', 9668)\n","('!', 8872)\n","('the', 8011)\n","('t', 7411)\n","('to', 6929)\n","('a', 6582)\n","('that', 5992)\n","('no', 4931)\n","('me', 4839)\n","('do', 4745)\n","('is', 4434)\n","('don', 3577)\n","('are', 3503)\n","('he', 3413)\n","('yes', 3384)\n","('m', 3382)\n","('not', 3252)\n","('we', 3252)\n","('know', 3171)\n","('re', 2965)\n","('your', 2809)\n","('this', 2726)\n","('yeah', 2708)\n","('in', 2678)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p52g6zoE3b8R"},"source":["Print a couple of sentences to verify that the vocabulary is working as intended."]},{"cell_type":"code","metadata":{"id":"BIZG1fmA1BED","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618943949952,"user_tz":240,"elapsed":672,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"9da56022-6fe4-4af0-ae04-57616fd7fc1c"},"source":["for src, tgt in all_conversations[:3]:\n","    sentence = tgt\n","    word_tokens = vocab.tokenized_sentence(sentence)\n","    # Automatically adds bos_id and eos_id before and after sentence ids respectively\n","    word_ids = vocab.get_ids_from_sentence(sentence)\n","    print(sentence)\n","    print(word_tokens)\n","    print(word_ids)\n","    print(vocab.decode_sentence_from_ids(word_ids))\n","    print()\n","\n","word = \"the\"\n","word_id = vocab.word_to_id[word]\n","print(f\"Word = {word}\")\n","print(f\"Word ID = {word_id}\")\n","print(f\"Word decoded from ID = {vocab.decode_sentence_from_ids([word_id])}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["where ?\n","['<s>', 'where', '?', '</s>']\n","[1, 6, 7, 2]\n","where ?\n","\n","you re sweet .\n","['<s>', 'you', 're', 'sweet', '.', '</s>']\n","[1, 8, 15, 16, 5, 2]\n","you re sweet .\n","\n","looks like things worked out tonight huh ?\n","['<s>', 'looks', 'like', 'things', 'worked', 'out', 'tonight', 'huh', '?', '</s>']\n","[1, 18, 19, 20, 21, 22, 23, 24, 7, 2]\n","looks like things worked out tonight huh ?\n","\n","Word = the\n","Word ID = 47\n","Word decoded from ID = the\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jVbxmW8N3qL4"},"source":["## Dataset Prepration (5 points)"]},{"cell_type":"markdown","metadata":{"id":"rre6BLuah8w-"},"source":["We will use built-in dataset utilities, `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`, to get batched data readily useful for training."]},{"cell_type":"code","metadata":{"id":"92DIBSL43pJT"},"source":["class SingleTurnMovieDialog_dataset(Dataset):\n","    \"\"\"Single-Turn version of Cornell Movie Dialog Cropus dataset.\"\"\"\n","\n","    def __init__(self, conversations, vocab, device):\n","        \"\"\"\n","        Args:\n","            conversations: list of tuple (src_string, tgt_string) \n","                         - src_string: String of the source sentence\n","                         - tgt_string: String of the target sentence\n","            vocab: Vocabulary object that contains the mapping of \n","                    words to indices\n","            device: cpu or cuda\n","        \"\"\"\n","        self.conversations = conversations\n","        self.vocab = vocab\n","        self.device = device\n","\n","        def encode(src, tgt):\n","            src_ids = self.vocab.get_ids_from_sentence(src)\n","            tgt_ids = self.vocab.get_ids_from_sentence(tgt)\n","            return (src_ids, tgt_ids)\n","\n","        # We will pre-tokenize the conversations and save in id lists for later use\n","        self.tokenized_conversations = [encode(src, tgt) for src, tgt in self.conversations]\n","        \n","    def __len__(self):\n","        return len(self.conversations)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        return {\"conv_ids\":self.tokenized_conversations[idx], \"conv\":self.conversations[idx]}\n","\n","def collate_fn(data):\n","    \"\"\"Creates mini-batch tensors from the list of tuples (src_seq, trg_seq).\n","    We should build a custom collate_fn rather than using default collate_fn,\n","    because merging sequences (including padding) is not supported in default.\n","    Seqeuences are padded to the maximum length of mini-batch sequences (dynamic padding).\n","    Args:\n","        data: list of dicts {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, trg_str)}.\n","            - src_ids: list of src piece ids; variable length.\n","            - tgt_ids: list of tgt piece ids; variable length.\n","            - src_str: String of src\n","            - tgt_str: String of tgt\n","    Returns: dict { \"conv_ids\":     (src_ids, tgt_ids), \n","                    \"conv\":         (src_str, tgt_str), \n","                    \"conv_tensors\": (src_seqs, tgt_seqs)}\n","            src_seqs: torch tensor of shape (src_padded_length, batch_size).\n","            trg_seqs: torch tensor of shape (tgt_padded_length, batch_size).\n","            src_padded_length = length of the longest src sequence from src_ids\n","            tgt_padded_length = length of the longest tgt sequence from tgt_ids\n","    \"\"\"\n","    # Sort conv_ids based on decreasing order of the src_lengths.\n","    # This is required for efficient GPU computations.\n","    src_ids = [torch.LongTensor(e[\"conv_ids\"][0]) for e in data]\n","    tgt_ids = [torch.LongTensor(e[\"conv_ids\"][1]) for e in data]\n","    src_str = [e[\"conv\"][0] for e in data]\n","    tgt_str = [e[\"conv\"][1] for e in data]\n","    data = list(zip(src_ids, tgt_ids, src_str, tgt_str))\n","    data.sort(key=lambda x: len(x[0]), reverse=True)\n","    src_ids, tgt_ids, src_str, tgt_str = zip(*data)\n","\n","\n","    # Pad the src_ids and tgt_ids using token pad_id to create src_seqs and tgt_seqs\n","    \n","    # Implementation tip: You can use the nn.utils.rnn.pad_sequence utility\n","    # function to combine a list of variable-length sequences with padding.\n","    \n","    # YOUR CODE HERE\n","    src_seqs = pad_sequence(src_ids, batch_first=False, padding_value=pad_id)\n","    tgt_seqs = pad_sequence(tgt_ids, batch_first=False, padding_value=pad_id)\n","\n","    return {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, tgt_str), \"conv_tensors\":(src_seqs.to(device), tgt_seqs.to(device))}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAQ26bgF4GT1"},"source":["# Create the DataLoader for all_conversations\n","dataset = SingleTurnMovieDialog_dataset(all_conversations, vocab, device)\n","\n","batch_size = 5\n","\n","data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n","                               shuffle=True, collate_fn=collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DC5oaHF4Mn6"},"source":["Let's test a batch of data to make sure everything is working as intended"]},{"cell_type":"code","metadata":{"id":"y8vYxSJl4NNp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618943960080,"user_tz":240,"elapsed":512,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"6d3ce92d-83c3-456b-d353-d1b629b02e95"},"source":["# Test one batch of training data\n","first_batch = next(iter(data_loader))\n","print(f\"Testing first training batch of size {len(first_batch['conv'][0])}\")\n","print(f\"List of source strings:\")\n","print_list(first_batch[\"conv\"][0])\n","print(f\"Tokenized source ids:\")\n","print_list(first_batch[\"conv_ids\"][0])\n","print(f\"Padded source ids as tensor (shape {first_batch['conv_tensors'][0].size()}):\")\n","print(first_batch[\"conv_tensors\"][0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testing first training batch of size 5\n","List of source strings:\n","sorry i m way behind .\n","you re shittin me .\n","that was good .\n","what s that ?\n",". . .tunnel !\n","\n","Tokenized source ids:\n","tensor([   1,  392,   54,  164,  271, 1006,    5,    2])\n","tensor([   1,    8,   15, 4813,   75,    5,    2])\n","tensor([ 1, 30, 89, 45,  5,  2])\n","tensor([ 1, 44, 31, 30,  7,  2])\n","tensor([   1,    5,    5, 6336,   58,    2])\n","\n","Padded source ids as tensor (shape torch.Size([8, 5])):\n","tensor([[   1,    1,    1,    1,    1],\n","        [ 392,    8,   30,   44,    5],\n","        [  54,   15,   89,   31,    5],\n","        [ 164, 4813,   45,   30, 6336],\n","        [ 271,   75,    5,    7,   58],\n","        [1006,    5,    2,    2,    2],\n","        [   5,    2,    0,    0,    0],\n","        [   2,    0,    0,    0,    0]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fHMAx6Zym_NS"},"source":["## Baseline Seq2Seq model (25 points)"]},{"cell_type":"markdown","metadata":{"id":"WcjfWInLnIsz"},"source":["With the training `Dataset` and `DataLoader` ready, we can implement our Seq2Seq baseline model. \n","\n","The model will consist of\n","1. Shared embedding layer between encoder and decoder that converts the input sequence of word ids to dense embedding representations\n","2. Bidirectional GRU encoder that encodes the embedded source sequence into hidden representation\n","3. GRU decoder that predicts target sequence using final encoder hidden representation"]},{"cell_type":"code","metadata":{"id":"s4QitXM9Szr9"},"source":["class Seq2seqBaseline(nn.Module):\n","    def __init__(self, vocab, emb_dim = 300, hidden_dim = 300, num_layers = 2, dropout=0.1):\n","        super().__init__()\n","\n","        # Initialize your model's parameters here. To get started, we suggest\n","        # setting all embedding and hidden dimensions to 300, using encoder and\n","        # decoder GRUs with 2 layers, and using a dropout rate of 0.1.\n","\n","        # Implementation tip: To create a bidirectional GRU, you don't need to\n","        # create two GRU networks. Instead use nn.GRU(..., bidirectional=True).\n","        \n","        self.num_words = num_words = vocab.num_words\n","        self.emb_dim = emb_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        # YOUR CODE HERE\n","\n","        self.embedding = torch.nn.Embedding(num_words, emb_dim)\n","        self.encoder_gru = torch.nn.GRU(emb_dim, hidden_size=hidden_dim, num_layers=2, dropout=dropout, bidirectional=True)\n","        self.decoder_gru = torch.nn.GRU(emb_dim, hidden_size=hidden_dim, num_layers=2, dropout=dropout)\n","        self.fc = torch.nn.Linear(hidden_dim, num_words)\n","    \n","\n","    def encode(self, source):\n","        \"\"\"Encode the source batch using a bidirectional GRU encoder.\n","\n","        Args:\n","            source: An integer tensor with shape (max_src_sequence_length,\n","                batch_size) containing subword indices for the source sentences.\n","\n","        Returns:\n","            A tuple with three elements:\n","                encoder_output: The output hidden representation of the encoder \n","                    with shape (max_src_sequence_length, batch_size, hidden_size).\n","                    Can be obtained by adding the hidden representations of both \n","                    directions of the encoder bidirectional GRU. \n","                encoder_mask: A boolean tensor with shape (max_src_sequence_length,\n","                    batch_size) indicating which encoder outputs correspond to padding\n","                    tokens. Its elements should be True at positions corresponding to\n","                    padding tokens and False elsewhere.\n","                encoder_hidden: The final hidden states of the bidirectional GRU \n","                    (after a suitable projection) that will be used to initialize \n","                    the decoder. This should be a tensor h_n with shape \n","                    (num_layers, batch_size, hidden_size). Note that the hidden \n","                    state returned by the bi-GRU cannot be used directly. Its \n","                    initial dimension is twice the required size because it \n","                    contains state from two directions.\n","\n","        The first two return values are not required for the baseline model and will\n","        only be used later in the attention model. If desired, they can be replaced\n","        with None for the initial implementation.\n","        \"\"\"\n","\n","        # Implementation tip: consider using packed sequences to more easily work\n","        # with the variable-length sequences represented by the source tensor.\n","        # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n","\n","        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n","\n","        # Implementation tip: there are many simple ways to combine the forward\n","        # and backward portions of the final hidden state, e.g. addition, averaging,\n","        # or a linear transformation of the appropriate size. Any of these\n","        # should let you reach the required performance.\n","\n","        # Compute a tensor containing the length of each source sequence.\n","        source_lengths = torch.sum(source != pad_id, axis=0).cpu()\n","       \n","        # YOUR CODE HERE\n","        input_embedding = self.embedding(source).cuda()\n","        gru_output, gru_hidden = self.encoder_gru(input_embedding)\n","\n","        # Original(seq_len, batch, num_directions * hidden_size)\n","        # Reshaped(seq_len, batch, num_directions, hidden_size)\n","        output_view = gru_output.view(source.shape[0], source.shape[1], 2, self.hidden_dim)    \n","        encoder_output = output_view[:, :, 0, :] + output_view[:, :, 1, :]\n","\n","        # Original(num_layers * num_directions, batch, hidden_size)\n","        # Reshaped(num_layers, num_directions, batch, hidden_size)\n","        hn_view = gru_hidden.view(self.num_layers, 2, source.shape[1], self.hidden_dim)    \n","        encoder_hidden = hn_view[:, 0, :, :] + hn_view[:, 1, :, :]\n","\n","        encoder_mask = source == pad_id\n","\n","        return (encoder_output, encoder_mask, encoder_hidden)\n","\n","\n","    def decode(self, decoder_input, last_hidden, encoder_output, encoder_mask):\n","        \"\"\"Run the decoder GRU for one decoding step from the last hidden state.\n","\n","        The third and fourth arguments are not used in the baseline model, but are\n","        included for compatibility with the attention model in the next section.\n","\n","        Args:\n","            decoder_input: An integer tensor with shape (1, batch_size) containing \n","                the subword indices for the current decoder input.\n","            last_hidden: A pair of tensors h_{t-1} representing the last hidden\n","                state of the decoder, each with shape (num_layers, batch_size,\n","                hidden_size). For the first decoding step the last_hidden will be \n","                encoder's final hidden representation.\n","            encoder_output: The output of the encoder with shape\n","                (max_src_sequence_length, batch_size, hidden_size).\n","            encoder_mask: The output mask from the encoder with shape\n","                (max_src_sequence_length, batch_size). Encoder outputs at positions\n","                with a True value correspond to padding tokens and should be ignored.\n","\n","        Returns:\n","            A tuple with three elements:\n","                logits: A tensor with shape (batch_size,\n","                    vocab_size) containing unnormalized scores for the next-word\n","                    predictions at each position.\n","                decoder_hidden: tensor h_n with the same shape as last_hidden \n","                    representing the updated decoder state after processing the \n","                    decoder input.\n","                attention_weights: This will be implemented later in the attention\n","                    model, but in order to maintain compatible type signatures, we also\n","                    include it here. This can be None or any other placeholder value.\n","        \"\"\"\n","\n","        # These arguments are not used in the baseline model.\n","        del encoder_output\n","        del encoder_mask\n","    \n","        # YOUR CODE HERE\n","        input_embedding = self.embedding(decoder_input)\n","\n","        gru_output, gru_hidden = self.decoder_gru(input_embedding, last_hidden)\n","        # Just want last layer probs for logits other is both layers hiddens\n","\n","        logits = self.fc(gru_output)  \n","\n","        return (logits, gru_hidden, None)\n","\n","    def compute_loss(self, source, target):\n","        \"\"\"Run the model on the source and compute the loss on the target.\n","\n","        Args:\n","            source: An integer tensor with shape (max_source_sequence_length,\n","                batch_size) containing subword indices for the source sentences.\n","            target: An integer tensor with shape (max_target_sequence_length,\n","                batch_size) containing subword indices for the target sentences.\n","\n","        Returns:\n","            A scalar float tensor representing cross-entropy loss on the current batch\n","            divided by the number of target tokens in the batch.\n","            Many of the target tokens will be pad tokens. You should mask the loss \n","            from these tokens using appropriate mask on the target tokens loss.\n","        \"\"\"\n","\n","        # Implementation tip: don't feed the target tensor directly to the decoder.\n","        # To see why, note that for a target sequence like <s> A B C </s>, you would\n","        # want to run the decoder on the prefix <s> A B C and have it predict the\n","        # suffix A B C </s>.\n","\n","        # You may run self.encode() on the source only once and decode the target \n","        # one step at a time.\n","\n","        # YOUR CODE HERE\n","        loss = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n","\n","        encoder_output,encoder_mask,last_hidden = self.encode(source)\n","\n","        total_loss = 0.0\n","        for i in range(target.shape[0]-1):\n","          logits,last_hidden,attention_weights = self.decode(target[i].view(1,target.size()[1]), last_hidden, encoder_output, encoder_mask)\n","          # The ith word in your prediction going into the decoder corresponds to the i+1th word in your decoder targets\n","          word_loss = loss(logits.squeeze(),target[i+1]) # ignore index should deal with padding\n","          total_loss += word_loss\n","        \n","        nonpad_tokens = torch.sum(torch.sum(source != pad_id, axis=0).cpu()).cpu()\n","        return total_loss / nonpad_tokens\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkcHo-tqsmaH"},"source":["We provide a training loop for training the model. You are welcome to modify the training loop by adjusting the learning rate or changing optmization settings.\n","\n","**Important:** During our testing we found that training the encoder and decoder with different learning rates is crucial for getting good performance over the small dialog corpus. Specifically, the decoder parameter learning rate should be 5 times the encoder parameter learning rate. Hence, add the encoder parameter variable names in the `encoder_parameter_names` as a list. For example, if encoder is using `self.embedding_layer` and `self.encoder_gru` layer then the `encoder_parameter_names` should be `['embedding_layer', 'encoder_gru']` "]},{"cell_type":"code","metadata":{"id":"rrFiSD1sZCUN"},"source":["def train(model, data_loader, num_epochs, model_file, learning_rate=0.0001):\n","    \"\"\"Train the model for given number of epochs and save the trained model in \n","    the final model_file.\n","    \"\"\"\n","\n","    decoder_learning_ratio = 5.0\n","    \n","    encoder_parameter_names = ['embedding', 'encoder_gru']\n","             \n","    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n","    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n","    encoder_params = [e[1] for e in encoder_named_params]\n","    decoder_params = [e[1] for e in decoder_named_params]\n","    optimizer = torch.optim.AdamW([{'params': encoder_params},\n","                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n","    \n","    clip = 50.0\n","    for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n","        # print(f\"Total training instances = {len(train_dataset)}\")\n","        # print(f\"train_data_loader = {len(train_data_loader)} {1180 > len(train_data_loader)/20}\")\n","        with tqdm.notebook.tqdm(\n","                data_loader,\n","                desc=\"epoch {}\".format(epoch + 1),\n","                unit=\"batch\",\n","                total=len(data_loader)) as batch_iterator:\n","            model.train()\n","            total_loss = 0.0\n","            for i, batch_data in enumerate(batch_iterator, start=1):\n","                source, target = batch_data[\"conv_tensors\"]\n","                optimizer.zero_grad()\n","                loss = model.compute_loss(source, target)\n","                total_loss += loss.item()\n","                loss.backward()\n","                # Gradient clipping before taking the step\n","                _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n","                optimizer.step()\n","\n","                batch_iterator.set_postfix(mean_loss=total_loss / i, current_loss=loss.item())\n","    # Save the model after training         \n","    torch.save(model.state_dict(), model_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIfCFLQcvQQn"},"source":["We can now train the baseline model.\n","\n","A correct implementation should get a average train loss of < 3.00  \n","The code will automatically save and download the model at the end of training."]},{"cell_type":"code","metadata":{"id":"MVqQnn59ZuHj","colab":{"base_uri":"https://localhost:8080/","height":370,"referenced_widgets":["d7ece88333da4a3e9ed51578b34cdf4b","184fd0001f1145b780a84d9003086c22","c1544475e585422ba155d155e945318d","f23806f9493544ae9f28b112695bcf94","04b94c61afca442a9f13456d0fd6266f","7bd366b52f074a958f7f64a5e1997c35","a7433cb5185b4e9097ae1e20efada18b","e58a3b45a0fd47d89b54b02397d0e152","f41b3485991144709127e1287787057f","5ef0cbc8dc80469fac2d1108aef716d5","31b19212cbbb417cb90891dca05a9e9c","3155b8b5ab9b445192dd094ff6fd3d26","6b5c643cd7a34026becac6475a588435","b6f31f619f874aa788212905ca961f35","a762e92ccdb04986b60cc894f721faaf","75ebb209ce3d433e828516f1d1a99df2","ce20f7c49ad94a75be7e4276f5f88897","4677f3b970c94a0a825014c7e56c3b6c","778b49546c644a2989e7da6d13beb6ee","32f1c9b66af44fe3bf283269889dc3d8","ebf57017cf8d4ff2b48c17b164608ff1","e65ecdd917504b229b69439ae16b6cee","e8e0de3b426e47aa9a5bfe87a237e0af","667386ce5ab2420ba05d8eeef0f02993","d24f7487374d43eb95dd5c418e4534b4","3218874f8197410eb01ea94c9488fd8c","b08d0a05d4bd44c79ceb6228fe035ea8","aee644a9b163487892063275b0437d84","60be7bd7270e4895ae664e8e7fbec951","c44142ec5ef9460994c5562ad7c24fe3","0ad8b24747a247a9ad2a92e7d6416231","ba6b8658500e4dcbb2cd2eda1ff767a2","d4a9114b5d7e4370aab82211bcb042a5","6311cd554f394767af84dd38f8cd81e5","2c30048e0c46435ea4acab6cd7e464bb","16cafd626d0b477db67f7fc120304091","87fa5ba4a79e457e9193cfce2a232d9d","6ba94e2c032d4ac3a7e8682cae2a495e","b3fe4a7898bf4349965ae20e6c0045ef","e6f2539e28334a97ba15158c5ab6da55","4fa5f7d13b6f478496e4c6ef05fc1f65","91e9aea2f05d4280992f8a352a7f33eb","21b15e9dfe5f4391afc1e914e7dc9e00","6ea14f3f81e54dc0ab2f64898e2609bc","1f52aa54e12843c9a4a0960b1315b479","8728a8c48a0c4be88862d527e43c4cd3","ca412840766d412b8c26c867fd41bd0e","ef7058e3b7c24687addeaad6953135e4","4e1b737364094188a562939ab3b901de","87d3bb8fcb7847ebbf78750063a1844e","61d0147c63574248a02ce0fd7a7d26f6","2a4225e4151b4147990ff81f7652ada8","b2777706be844dc8a422017138f0bcd9","9d181849bf944a11bb549be126784cc7","8a34cf0e4ec4471aac8d23f995a8cf29","bc69f4e60b534fd4abcb035a99afa78f"]},"executionInfo":{"status":"ok","timestamp":1618943656353,"user_tz":240,"elapsed":325118,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"7d376d95-7292-4903-ae2b-0baea699461e"},"source":["# You are welcome to adjust these parameters based on your model implementation.\n","num_epochs = 6\n","batch_size = 64\n","# Reloading the data_loader to increase batch_size\n","data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n","                               shuffle=True, collate_fn=collate_fn)\n","\n","baseline_model = Seq2seqBaseline(vocab).to(device)\n","train(baseline_model, data_loader, num_epochs, \"baseline_model.pt\")\n","# Download the trained model to local for future use\n","files.download('baseline_model.pt')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7ece88333da4a3e9ed51578b34cdf4b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='training', max=6.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f41b3485991144709127e1287787057f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 1', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce20f7c49ad94a75be7e4276f5f88897","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 2', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d24f7487374d43eb95dd5c418e4534b4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 3', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4a9114b5d7e4370aab82211bcb042a5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 4', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fa5f7d13b6f478496e4c6ef05fc1f65","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 5', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e1b737364094188a562939ab3b901de","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 6', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_f390442b-8ff7-4698-9729-ecc313a55335\", \"baseline_model.pt\", 33743644)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"U83z2yBBw8_N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618942071404,"user_tz":240,"elapsed":434,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"bff96eb4-dd9a-4e73-f365-d394b3e0485b"},"source":["# Reload the model from the model file. \n","# Useful when you have already trained and saved the model\n","baseline_model = Seq2seqBaseline(vocab).to(device)\n","baseline_model.load_state_dict(torch.load(\"baseline_model.pt\", map_location=device))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"Z8vuvtV7PzJf"},"source":["## Greedy Search (10 points)"]},{"cell_type":"markdown","metadata":{"id":"zOqRfnqrxf3f"},"source":["For evaluation, we also need to be able to generate entire strings from the model. We'll first define a greedy inference procedure here. Later on, we'll implement beam search.\n"]},{"cell_type":"code","metadata":{"id":"HG-_G8wZdlJh"},"source":["def predict_greedy(model, sentence, max_length=100):\n","    \"\"\"Make predictions for the given input using greedy inference.\n","    \n","    Args:\n","        model: A sequence-to-sequence model.\n","        sentence: A input string.\n","        max_length: The maximum length at which to truncate outputs in order to\n","            avoid non-terminating inference.\n","    \n","    Returns:\n","        Model's predicted greedy response for the input, represented as string.\n","    \"\"\"\n","\n","    # You should make only one call to model.encode() at the start of the function, \n","    # and make only one call to model.decode() per inference step.\n","    model.eval()\n","\n","    # YOUR CODE HERE\n","    sentence_ids = torch.tensor(vocab.get_ids_from_sentence(sentence))\n","\n","    encoder_output,encoder_mask,last_hidden = model.encode(torch.unsqueeze(sentence_ids,1).cuda())\n","\n","    output_ids = torch.zeros((max_length), dtype=torch.long)\n","    output_ids[0] = bos_id # start of sentence\n","\n","    i = 0\n","    while i < max_length and not int(output_ids[i]) == eos_id:\n","      this_hurts = torch.unsqueeze(torch.unsqueeze(output_ids[i],0),1)\n","      logits,last_hidden,attention_weights =  model.decode(this_hurts.cuda(), last_hidden.cuda(), encoder_output, encoder_mask)\n","      output_ids[i+1] = logits.argmax(dim=2)\n","      i += 1\n","\n","    return vocab.decode_sentence_from_ids(output_ids.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zAsOGMw_kHHO"},"source":["Let's chat interactively with our trained baseline Seq2Seq dialog model and save the generated conversations for submission (please make sure to keep the conversations in your submission [\"PG-13\"](https://en.wikipedia.org/wiki/Motion_Picture_Association_film_rating_system)). We will reuse the conversational inputs while testing Seq2Seq + Attention model.\n","\n","Note: enter \"q\" or \"quit\" to end the interactive chat"]},{"cell_type":"code","metadata":{"id":"SWlUDUMvfmRy"},"source":["def chat_with_model(model, mode=\"greedy\"):\n","    if mode == \"beam\":\n","        predict_f = predict_beam\n","    else:\n","        predict_f = predict_greedy\n","    chat_log = list()\n","    input_sentence = ''\n","    while(1):\n","        # Get input sentence\n","        input_sentence = input('Input > ')\n","        # Check if it is quit case\n","        if input_sentence == 'q' or input_sentence == 'quit': break\n","        \n","        generation = predict_f(model, input_sentence)\n","        if mode == \"beam\":\n","            generation = generation[0]\n","        print('Greedy Response:', generation)\n","        print()\n","        chat_log.append((input_sentence, generation))\n","    return chat_log"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hm_WWqqnf_XF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618944033032,"user_tz":240,"elapsed":55176,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"60243cc2-2e15-41da-a3e9-9758bdb5154b"},"source":["baseline_chat = chat_with_model(baseline_model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input > Why do you hate me?\n","Greedy Response: i don t know .\n","\n","Input > You can give me a better answer than that.\n","Greedy Response: what ?\n","\n","Input > What do you mean what!\n","Greedy Response: i m sorry .\n","\n","Input > You should be.\n","Greedy Response: i m not sure .\n","\n","Input > Stupid computer\n","Greedy Response: i ll be there .\n","\n","Input > q\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WdShLs1VkgAb"},"source":["## Seq2Seq + Attention Model (15 points)"]},{"cell_type":"markdown","metadata":{"id":"b4N4VGarkmH5"},"source":["# Next, we extend the baseline model to include an attention mechanism in the decoder. With attention mechanism, the model doesn't need to encode the input into a fixed dimensional hidden representation. Rather, it creates a new context vector for each turn that is a weighted sum of encoder hidden representation. \n","\n","Your implementation can use any attention mechanism to get weight distribution over the source words. One simple way to include attention in decoder goes as follows (reminder: the decoder processed one token at a time),\n","1. Process the current decoder_input through embedding layer and decoder GRU layer.\n","2. Use the current decoder token representation, $d$ of shape $(1 * b * h)$ and encoder representation, $e_1, \\dots, e_n$ or shape $(n * b * h)$, where $n$ is max_src_length after padding) to compute attention score matrix of shape $(b * n)$. There are multiple options to compute this score matrix. A few of such options are available in [the table provided in this blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)\n","3. Normalize the attention scores $(b * n)$ so that they sum up to $1.0$ by taking a `softmax` over the second dimention. \n","\n","After computing the normalized attention distribution, take a weighted sum of the encoder outputs to obtain the attention context $c = \\sum_i w_i e_i$, and add this to the decoder output $d$ to obtain the final representation to be passed to the vocabulary projection layer (you may need another linear layer to make the sizes match before adding $c$ and $d$)."]},{"cell_type":"code","metadata":{"id":"QTJUmIH2iAI7"},"source":["class Seq2seqAttention(Seq2seqBaseline):\n","    def __init__(self, vocab):\n","        super().__init__(vocab)\n","\n","        # Initialize any additional parameters needed for this model that are not\n","        # already included in the baseline model.\n","        \n","        # YOUR CODE HERE\n","        self.attention_fc = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n","        self.attention_softmax = torch.nn.Softmax(dim=2)\n","\n","    def decode(self, decoder_input, last_hidden, encoder_output, encoder_mask):\n","        \"\"\"Run the decoder GRU for one decoding step from the last hidden state.\n","\n","        The third and fourth arguments are not used in the baseline model, but are\n","        included for compatibility with the attention model in the next section.\n","\n","        Args:\n","            decoder_input: An integer tensor with shape (1, batch_size) containing \n","                the subword indices for the current decoder input.\n","            last_hidden: A pair of tensors h_{t-1} representing the last hidden\n","                state of the decoder, each with shape (num_layers, batch_size,\n","                hidden_size). For the first decoding step the last_hidden will be \n","                encoder's final hidden representation.\n","            encoder_output: The output of the encoder with shape\n","                (max_src_sequence_length, batch_size, hidden_size).\n","            encoder_mask: The output mask from the encoder with shape\n","                (max_src_sequence_length, batch_size). Encoder outputs at positions\n","                with a True value correspond to padding tokens and should be ignored.\n","\n","        Returns:\n","            A tuple with three elements:\n","                logits: A tensor with shape (batch_size,\n","                    vocab_size) containing unnormalized scores for the next-word\n","                    predictions at each position.\n","                decoder_hidden: tensor h_n with the same shape as last_hidden \n","                    representing the updated decoder state after processing the \n","                    decoder input.\n","                attention_weights: A tensor with shape (batch_size, \n","                    max_src_sequence_length) representing the normalized\n","                    attention weights. This should sum to 1 along the last dimension.\n","        \"\"\"\n","\n","        # YOUR CODE HERE\n","        # From above just reorganized\n","        # 1. Process the current decoder_input through embedding layer and decoder GRU layer.\n","        # 2. Use the current decoder token representation, d of shape (1∗b∗h) and encoder representation,\n","        # e1,…,en or shape (n∗b∗h), where n is max_src_length after padding) to compute attention score matrix of shape (b∗n). \n","        # There are multiple options to compute this score matrix. A few of such options are available in the table provided in this blog\n","        # 3. Normalize the attention scores (b∗n) so that they sum up to 1.0 by taking a softmax over the second dimension.\n","        # 4. Take a weighted sum of the encoder outputs to obtain the attention context c=∑iwiei\n","        # 5. Add attention context c to the decoder output d to obtain the merged representation to be passed to the vocabulary projection layer\n","        # (you may need another linear layer to make the sizes match before adding c and d). \n","        # 6. Perform vocab projection\n","\n","        batch_size = decoder_input.size()[1]\n","\n","        # 1. Process the current decoder_input through embedding layer and decoder GRU layer.\n","        input_embedding = self.embedding(decoder_input)\n","        decoder_output, decoder_hidden = self.decoder_gru(input_embedding, last_hidden)\n","\n","        # 2. Use the current decoder token representation, d of shape (1∗b∗h) and encoder representation,\n","        # e1,…,en or shape (n∗b∗h), where n is max_src_length after padding) to compute attention score matrix of shape (b∗n).\n","        # doing dot attention\n","        decoder_output_dot_reshaped = decoder_output.permute(1,0,2)\n","        encoder_output_dot_reshaped = encoder_output.permute(1,2,0)\n","        attention_scores = torch.bmm(decoder_output_dot_reshaped, encoder_output_dot_reshaped)\n","\n","        # 3. Normalize the attention scores (b,1,n) so that they sum up to 1.0 by taking a softmax over the second dimension.\n","        attention_weights = self.attention_softmax(attention_scores)\n","\n","        # 4. Take a weighted sum of the encoder outputs to obtain the attention context c=∑iwiei\n","        # So we want attention_weights (b,1,n) times encoder_output (n,b,h) \n","        # So we just need to rearrange encoder_output to (b,n,h)\n","        encoder_output_weight_reshape = encoder_output.permute(1,0,2)\n","        # Then mult\n","        attention_vec = torch.bmm(attention_weights, encoder_output_weight_reshape)\n","\n","        # 5. Add attention context c to the decoder output d to obtain the final representation to be passed to the vocabulary projection layer\n","        merged = torch.cat((attention_vec, decoder_output.permute(1,0,2)), dim=2)\n","        \n","        # 6. Perform vocab projection\n","        final_representation = self.attention_fc(merged).view(batch_size, 1, self.hidden_dim)\n","        logits = self.fc(final_representation)\n","\n","        return (logits, decoder_hidden, attention_weights.squeeze())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTfMgmoHr_zQ"},"source":["We can now train the attention model.\n","\n","A correct implementation should also get an average train loss of < 3.00  \n","The code will automatically save and download the model at the end of training.\n","\n","It may happen that the baseline model achieves lower loss than attention model. This is because our dataset is very small and the attention model may be over parameterized for our toy dataset. Regardless, we would consider this as acceptable submission if the attention model generated responses look comparable to the baseline model."]},{"cell_type":"code","metadata":{"id":"RzPC4ADNkNgm","colab":{"base_uri":"https://localhost:8080/","height":470,"referenced_widgets":["c937daa32b874bdab737e084d5f9debb","222d404641274d9d8e61be4340478ebb","2ed3fd4cf0bd489581db2409a0e8c292","bf86944d00f74f7c893013945815b808","964baaed9d754828a50991c8c0fe94db","0ddbd671f5964d9ebdc0b7fc711001dc","24c6a88391574c649418f9b677608ce1","47113b9a46b941bbb15ac76014b35f6e","a54b3d9a10d24712ad011b27b76bd2c9","62114473e8a54d9d98df8354b5cf2d4e","bc82f9c1a5474e71b78a2c06542d76ec","bc1945b60c584fe7bb98fba3e127f8da","def3c09022504988a8f6b50379e95b55","12cad9c34c0f4a4d903ba05cfc998668","29eb432a456648f8a5bd44d47a6e5ec9","544d2e9877464e9d878fa1b1ce749997","f233312871d04888a2d5c355f0eaeeb7","c9a2838274224cb5ae7c70c87223be67","4f400f45e6044c1dbfa8b509bed866e1","99f0e65023934b058a5412bca39e1afc","04b4985e628d49a5acf55765f9ef8c25","87cdd241b3554df89a9b4ff3f01c4612","eae7e216c5744b73809046aa733954cb","0050c26bf6c3452d8ee90efb6b0ae9cb","879091214e344b7788df8dfd76f7c97c","b646a6e67ba94ee4847f7c2a2887f95e","b199b60c88a347358a542d994d381e3b","696a4942fc994be59e35522b63231200","5d1fa3c94ff2492d8aa255eea25275a1","3f813f24d16b4ba0903efe3beed0c3b6","4068655b0505460580bfcabb716809d0","1f7e82ee9151418997ebc8515fbcc645","5ce1f8e648c94f8ba6ac9d274a8e6bfe","5cb611bf81124d7f84a0edcd0ffbe239","45386a10ac2a4698a323b6d776ed72f1","94ae6ad1783d4e0f90fa4d0239357e09","a15df5aa2173465b8b73cea684234033","5f7dd1f66c3e427c929dbc0c55e200d2","77f6b20085834b2789a9aa206406937f","a991035743864f22b679964d71db99b1","23abef220fd54755bd2deb1402fc2d75","702e82fd639c421193e00cb63614d697","49786989b84147c99f3b97f9561d61ee","72237d5909b143b8b1bb17d141dc0257","f82dcac3b48f4b50897fddfbc7d02001","b0147a4be8ff4399a2dd96280f1ae596","430e701c88824b21ad8fa23640054286","ffea3f205c2e4d7a80fdf853b3a4f25d","db80c8115c4c47ec93716ffd0de8cd6c","64a5f65e533e4cc2a8e77bb8ab5b89af","b7925ddd778740c9ac04c4f442a4e3f2","4952657edf714eca85f40d9641a10e9d","6ba4147744d24502af2e6cbe7b278c0e","67488eb083d047a7b751bd34ed6ee860","aa3b3fbacf864b5791ca39ea24fd3210","3dcf1b84234242dd8e3ecc01aea8a0bc","06bb1e329f074326a5ec0265a530abf9","3842299df5c2467096ec82b9978b520f","9d88b6257a6946e2aac99eeec8d635c8","604d0736c1d14eba973ae8530eb83390","d7cf2b9878ab449ba6be8676e065cfa0","c180a891a58347048bcd39e55d3a3b33","2d50a98d275c4f39a1aac87f882acab4","2590b8109e8e498e87510bb627dd435f","11c748da38c4461884733c03b9f143d8","20714c0314e040ba9e4ae42d30b94e10","8823d9c4cfa34e158868e1a7fd95c0dc","3f87df90a7eb45678a51a82433ea877d","7a260134c53c4fb08728f254561eb459","edfc078456d44140bf38c91b16e136e1","b365c851f04641b08088bedbdf1f6a43","25cb33b2d51149579f8a959f2be6f6d2"]},"executionInfo":{"status":"ok","timestamp":1618937902491,"user_tz":240,"elapsed":831753,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"f67cb9f6-cacd-4f6e-b270-0d944a4e3e94"},"source":["# You are welcome to adjust these parameters based on your model implementation.\n","num_epochs = 8\n","batch_size = 64\n","\n","data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n","                               shuffle=True, collate_fn=collate_fn)\n","\n","attention_model = Seq2seqAttention(vocab).to(device)\n","train(attention_model, data_loader, num_epochs, \"attention_model.pt\")\n","# Download the trained model to local for future use\n","files.download('attention_model.pt')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c937daa32b874bdab737e084d5f9debb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='training', max=8.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a54b3d9a10d24712ad011b27b76bd2c9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 1', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f233312871d04888a2d5c355f0eaeeb7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 2', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"879091214e344b7788df8dfd76f7c97c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 3', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ce1f8e648c94f8ba6ac9d274a8e6bfe","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 4', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23abef220fd54755bd2deb1402fc2d75","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 5', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db80c8115c4c47ec93716ffd0de8cd6c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 6', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06bb1e329f074326a5ec0265a530abf9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 7', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11c748da38c4461884733c03b9f143d8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='epoch 8', max=830.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_42bca288-e3bb-45f5-97a5-f939189366eb\", \"attention_model.pt\", 34465454)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"t95mGiCCxaD0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618944056503,"user_tz":240,"elapsed":591,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"c4a38bb6-2c49-431e-d5ca-ccf4657d393f"},"source":["# Reload the model from the model file. \n","# Useful when you have already trained and saved the model\n","attention_model = Seq2seqAttention(vocab).to(device)\n","attention_model.load_state_dict(torch.load(\"attention_model.pt\", map_location=device))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"markdown","metadata":{"id":"Gt11szXtsruf"},"source":["Let's test the attention model on the same inputs as baseline model."]},{"cell_type":"code","metadata":{"id":"06W_twUH_fWj"},"source":["def test_conversations_with_model(model, conversational_inputs = None, include_beam = False):\n","    # Some predefined conversational inputs. \n","    # You may append more inputs at the end of the list, if you want to.\n","    basic_conversational_inputs = [\n","                                    \"hello.\",\n","                                    \"please share you bank account number with me\",\n","                                    \"i have never met someone more annoying that you\",\n","                                    \"i like pizza. what do you like?\",\n","                                    \"give me coffee, or i'll hate you\",\n","                                    \"i'm so bored. give some suggestions\",\n","                                    \"stop running or you'll fall hard\",\n","                                    \"what is your favorite sport?\",\n","                                    \"do you believe in a miracle?\",\n","                                    \"which sport team do you like?\"\n","    ]\n","    if not conversational_inputs:\n","        conversational_inputs = basic_conversational_inputs\n","    for input in conversational_inputs:\n","        print(f\"Input > {input}\")\n","        generation = predict_greedy(model, input)\n","        print('Greedy Response:', generation)\n","        if include_beam:\n","            # Also print the beam search responses from models\n","            generations = predict_beam(model, input)\n","            print('Beam Responses:')\n","            print_list(generations)\n","        print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-Z4hBdxp-RE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618944061819,"user_tz":240,"elapsed":729,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"90e46dfa-b520-4c06-e770-78390d06cfc0"},"source":["baseline_chat_inputs = [inp for inp, gen in baseline_chat]\n","attention_chat = test_conversations_with_model(attention_model, baseline_chat_inputs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input > Why do you hate me?\n","Greedy Response: yes i do .\n","\n","Input > You can give me a better answer than that.\n","Greedy Response: that s right .\n","\n","Input > What do you mean what!\n","Greedy Response: i don t know .\n","\n","Input > You should be.\n","Greedy Response: i m sorry .\n","\n","Input > Stupid computer\n","Greedy Response: yeah .\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"65xXyT8_twbK"},"source":["## Beam Search (20 points)"]},{"cell_type":"markdown","metadata":{"id":"kNP1C7JuHPhV"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"AQCpBhM3t10o"},"source":["Similar to greedy search, beam search generates one token at a time. However, rather than keeping only the single best hypothesis, we instead keep the top $k$ candidates at each time step. This is accomplished by computing the set of next-token extensions for each item on the beam and finding the top $k$ across all candidates according to total log-probability.\n","\n","Candidates that are finished should be extracted in a final list of `generations` and removed from the beam. This strategy is useful for doing re-ranking the beam candidates using alternate scorers (example, Maximum Mutual Information Objective from [Li et. al. 2015](https://arxiv.org/pdf/1510.03055.pdf)). For this assignment, you will re-rank the beam generations as follows,  \n","$final\\_score_i = \\frac{score_i}{|generation_i|^\\alpha}$, where $\\alpha \\in [0.5, 2]$.  \n","Terminate the search process once you have $k$ items in the `generations` list."]},{"cell_type":"code","metadata":{"id":"r9uz-n9RvbYm"},"source":["def predict_beam(model, sentence, k=5, max_length=100):\n","    \"\"\"Make predictions for the given inputs using beam search.\n","    \n","    Args:\n","        model: A sequence-to-sequence model.\n","        sentence: An input sentence, represented as string.\n","        k: The size of the beam.\n","        max_length: The maximum length at which to truncate outputs in order to\n","            avoid non-terminating inference.\n","    \n","    Returns:\n","        A list of k beam predictions. Each element in the list should be a string\n","        corresponding to one of the top k predictions for the corresponding input,\n","        sorted in descending order by its final score.\n","    \"\"\"\n","\n","    # Implementation tip: once an eos_token has been generated for any beam, \n","    # remove its subsequent predictions from that beam by adding a small negative \n","    # number like -1e9 to the appropriate logits. This will ensure that the \n","    # candidates are removed from the beam, as its probability will be very close\n","    # to 0. Using this method, uou will be able to reuse the beam of an already \n","    # finished candidate\n","\n","    # Implementation tip: while you are encouraged to keep your tensor dimensions\n","    # constant for simplicity (aside from the sequence length), some special care\n","    # will need to be taken on the first iteration to ensure that your beam\n","    # doesn't fill up with k identical copies of the same candidate.\n","    \n","    # You are welcome to tweak alpha\n","    alpha = 0.5\n","    model.eval()\n","    \n","    # YOUR CODE HERE\n","    sentence_ids = torch.tensor(vocab.get_ids_from_sentence(sentence))\n","\n","    encoder_output,encoder_mask,last_hidden = model.encode(torch.unsqueeze(sentence_ids,1).cuda())\n","\n","    output_ids = torch.zeros((max_length, k), dtype=torch.long)\n","    output_ids[0,:] = bos_id # set to start of sentence\n","\n","    # score of that output id as just a single word as a canidate\n","    global_score_tens = torch.zeros((max_length, k), dtype=torch.float) \n","\n","    # which K index this location in output_ids was preceeded by (what dim 1 slot or column of output_ids this came from)\n","    backpointer_tens = torch.zeros((max_length, k), dtype=torch.long) \n","\n","    generations = [] # final results\n","  \n","    # Do first step (first step away from start of sentence) here\n","    this_hurts = torch.unsqueeze(torch.unsqueeze(torch.tensor(1),0),1)\n","    logits,last_hidden,attention_weights = model.decode(this_hurts.cuda(), last_hidden.cuda(), encoder_output, encoder_mask)\n","    output_ids[1,:] = torch.topk(logits, k)[1] # word id\n","    global_score_tens[1,:] = F.log_softmax(torch.topk(logits, k)[0], dim=0) # logit value\n","    backpointer_tens[1,:] = 0 # all zeros anyways\n","    # Words shouldnt repeat because they all come from one call to the decode \n","    # (decode only returns prob from each word once doing it this way)\n","  \n","    # Hidden state for each of the k different previous words all have the same hidden state to start\n","    last_hidden_tens = last_hidden.repeat(k,1,1,1)\n","\n","    i = 2\n","    # Need cutoff for generations being full\n","    while i < max_length and len(generations) < k:\n","      # k by vocab size vector that contains the log prob that each word would\n","      # have for the local decision assuming k was the predecesor\n","      k_logits = torch.zeros((k, model.num_words))\n","\n","      for prev_k in range(k):\n","        this_hurts = torch.unsqueeze(torch.unsqueeze(output_ids[i-1,prev_k],0),1)\n","        logits,last_hidden,attention_weights =  model.decode(this_hurts.cuda(), last_hidden_tens[prev_k].cuda(), encoder_output, encoder_mask)\n","        last_hidden_tens[prev_k] = last_hidden # should be same dims\n","\n","        k_logits[prev_k,:] = F.log_softmax(logits.squeeze(), dim=0)\n","      \n","      # score of last k as log prob sized to k and then summed with all the candidates it produced\n","      k_logits = global_score_tens[i-1,:].repeat(model.num_words, 1).permute(1,0) + k_logits\n","\n","      eos_clear = False\n","      while eos_clear == False:\n","        score_tens, prev_tens = torch.max(k_logits, dim=0)\n","\n","        best_k = torch.topk(score_tens, k)[1] # word id\n","        output_ids[i,:] = best_k # word id\n","        global_score_tens[i,:] = torch.topk(score_tens, k)[0] # logit value\n","\n","        backpointer_tens[i,:] = prev_tens[best_k]\n","\n","        if eos_id not in best_k:\n","          eos_clear = True\n","        else:\n","          for k_ind in range(k):\n","            if best_k[k_ind] == eos_id: \n","              # Reconstruct sequence\n","              word_ids = []\n","              word_ids.append(int(output_ids[i,k_ind]))\n","              curr_k = backpointer_tens[i,k_ind]\n","\n","              j = i - 1\n","              while j > 0: \n","                word_ids.append(int(output_ids[j,curr_k]))\n","                curr_k = backpointer_tens[j,curr_k]\n","                j -= 1\n","\n","              word_ids.append(bos_id) # append start symbol\n","              word_ids.reverse()\n","              sentence = vocab.decode_sentence_from_ids(word_ids)\n","              # append tuple of sentence and score\n","              generations.append((sentence, global_score_tens[i,k_ind]))\n","\n","              # remove associated score from running\n","              k_logits[backpointer_tens[i,k_ind], :] = -1e9\n","\n","              # reweighting handled by loop\n","              if (len(generations) >= k):\n","                eos_clear = True\n","              break;\n","\n","      i += 1\n","\n","    generations = generations[:k]\n","\n","    # TODO FIX GENERATIONS HERE\n","    final_scores = [(element[0], float(element[1])/(float(len(element[0]))**alpha)) for element in generations]\n","    final_scores = sorted(final_scores, key=lambda tup: tup[1])\n","    final_results = [element[0] for element in final_scores]\n","    return final_results\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H-ugmcEJS2KR"},"source":[""]},{"cell_type":"code","metadata":{"id":"Z5latq5wAahX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618944072552,"user_tz":240,"elapsed":903,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"dfd6a6b9-f568-4000-8e3b-f288f387c16a"},"source":["test_conversations_with_model(baseline_model, include_beam=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input > hello.\n","Greedy Response: hello .\n","Beam Responses:\n","i m here ?\n","i m sorry .\n","you re right now ?\n","hi .\n","what ?\n","\n","\n","Input > please share you bank account number with me\n","Greedy Response: i m not sure .\n","Beam Responses:\n","i m .\n","i m sieu ?\n","no .\n","you re you know .\n","what ?\n","\n","\n","Input > i have never met someone more annoying that you\n","Greedy Response: i don t know .\n","Beam Responses:\n","i don t i do it .\n","what do you know . .\n","you re not .\n","no .\n","what ?\n","\n","\n","Input > i like pizza. what do you like?\n","Greedy Response: i m not .\n","Beam Responses:\n","it s wrong .\n","it s the difference .\n","do you like a little .\n","it s the matter ?\n","what ?\n","\n","\n","Input > give me coffee, or i'll hate you\n","Greedy Response: you re not gonna do anything .\n","Beam Responses:\n","no .\n","you re you re going to me\n","you re you re a lot .\n","i m sieu .\n","what ?\n","\n","\n","Input > i'm so bored. give some suggestions\n","Greedy Response: you re not going to be here ?\n","Beam Responses:\n","no .\n","that s too bad .\n","i m sieu .\n","that s not going to you ?\n","what ?\n","\n","\n","Input > stop running or you'll fall hard\n","Greedy Response: i m not .\n","Beam Responses:\n","it s not .\n","i m .\n","no .\n","it s not here .\n","what ?\n","\n","\n","Input > what is your favorite sport?\n","Greedy Response: i m not sure .\n","Beam Responses:\n","i m ?\n","it s a cop .\n","what do .\n","it s not going ?\n","what ?\n","\n","\n","Input > do you believe in a miracle?\n","Greedy Response: no .\n","Beam Responses:\n","i don t i .\n","i don t i ?\n","i m sieu .\n","sure .\n","what ?\n","\n","\n","Input > which sport team do you like?\n","Greedy Response: i m not .\n","Beam Responses:\n","it s not so .\n","it s not .\n","it s not a little ?\n","what do .\n","what ?\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J2OPC1FCAxLN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618944078582,"user_tz":240,"elapsed":845,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"bfa7cda7-8543-4d9a-c7c4-257855a85428"},"source":["test_conversations_with_model(attention_model, include_beam=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input > hello.\n","Greedy Response: hello .\n","Beam Responses:\n","what are you want to come ?\n","what do .\n","what are you are you ?\n","hi .\n","what ?\n","\n","\n","Input > please share you bank account number with me\n","Greedy Response: i m not .\n","Beam Responses:\n","yes i m sieu !\n","you re not .\n","yes i said ?\n","no .\n","what ?\n","\n","\n","Input > i have never met someone more annoying that you\n","Greedy Response: no\n","Beam Responses:\n","i don t you have\n","i don t wait ?\n","you re not ?\n","why not ?\n","why ?\n","\n","\n","Input > i like pizza. what do you like?\n","Greedy Response: like what ?\n","Beam Responses:\n","you like it !\n","what do you .\n","what do i mean ?\n","like what ?\n","what ?\n","\n","\n","Input > give me coffee, or i'll hate you\n","Greedy Response: i m sorry .\n","Beam Responses:\n","that s just kidding .\n","you re you .\n","that s not ?\n","no .\n","what ?\n","\n","\n","Input > i'm so bored. give some suggestions\n","Greedy Response: i m sorry .\n","Beam Responses:\n","you re you don t ?\n","i ll do you mean .\n","yes . . .i .\n","what ?\n","yes . . .\n","\n","\n","Input > stop running or you'll fall hard\n","Greedy Response: i m sorry i m sorry .\n","Beam Responses:\n","i don t .\n","it s just say .\n","it s just be cool ?\n","it s just be cool .\n","what ?\n","\n","\n","Input > what is your favorite sport?\n","Greedy Response: i don t know .\n","Beam Responses:\n","that s a cop ?\n","that s a cop .\n","that s got a nice .\n","i don t .\n","what ?\n","\n","\n","Input > do you believe in a miracle?\n","Greedy Response: yes .\n","Beam Responses:\n","i don t .\n","yeah i don t ?\n","yeah i don t .\n","no .\n","what ?\n","\n","\n","Input > which sport team do you like?\n","Greedy Response: i don t know .\n","Beam Responses:\n","i don t like it .\n","that s on the point with\n","that s on the story ?\n","i don t .\n","what ?\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j-CyLa-dO3pe"},"source":["## Automatic Evaluation (5 points)"]},{"cell_type":"markdown","metadata":{"id":"CJG6TYU6PDMH"},"source":["Automatic evaluation of chatbots is an active research area. For this assignment we are going to use 3 very simple evaluation metrics.\n","1. Average Length of the Responses\n","2. Distinct1 = proportion of unique unigrams / total unigrams\n","3. Distinct2 = proportion of unique bigrams / total bigrams  \n","You will evaluate your baseline and attention models by running the cells below."]},{"cell_type":"code","metadata":{"id":"-ZHWxMhnHW4g"},"source":["# Evaluate diversity of the models\n","def evaluate_diversity(model, mode=\"greedy\"):\n","    \"\"\"Evaluates the model's greedy or beam responses on eval_conversations\n","    \n","    Args:\n","        model: A sequence-to-sequence model.\n","        mode: \"greedy\" or \"beam\"\n","    \n","    Returns: avg_length, distinct1, distinct2\n","        avg_length: average length of the model responses\n","        distinct1: proportion of unique unigrams / total unigrams\n","        distinct2: proportion of unique bigrams / total bigrams\n","    \"\"\"\n","    if mode == \"beam\":\n","        predict_f = predict_beam\n","    else:\n","        predict_f = predict_greedy\n","    generations = list()\n","    for src, tgt in eval_conversations:\n","        generation = predict_f(model, src)\n","        if mode == \"beam\":\n","            generation = generation[0]\n","        generations.append(generation)\n","    # Calculate average length, distinct unigrams and bigrams from generations\n","    \n","    # YOUR CODE HERE\n","    generations_ids = [vocab.tokenized_sentence(generation) for generation in generations]\n","    print(generations_ids)\n","   \n","    total_unigrams = 0\n","    for generation in generations_ids:\n","      total_unigrams += (len(generation) - 2) # Bos and Eos removed\n","\n","    avg_length = float(total_unigrams) / float(len(generations_ids))\n","\n","    unigram_set = set()\n","    unigram_set.update([b for l in generations_ids for b in l])\n","\n","    distinct1 = float(len(unigram_set) - 2) / float(total_unigrams) # unique - 2 for start, and end of sentence\n","\n","    # For a sentence of length n you will have n-1 bigrams total\n","    # so total_unigrams - len(generations_ids) = total bigrams\n","    total_bigrams = total_unigrams - len(generations_ids)\n","\n","    # every element except last zipped with every element except the first\n","    bigrams_set = set()\n","    bigrams_set.update([b for l in generations for b in zip(l.lower().split(\" \")[:-1], l.lower().split(\" \")[1:])])\n","    distinct2 = float(len(bigrams_set)) / float(total_bigrams)\n","\n","    return avg_length, distinct1, distinct2\n","\n","    # For this toy dataset, we don't have a good baseline to compare against. \n","    # But, a correct implementation should at least get > 0.11  for both greedy\n","    # and beam decoding with attention and ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UErLX0EPHY2j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618944091259,"user_tz":240,"elapsed":3699,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"3b095143-f2b7-4cf2-bbff-4965735aa496"},"source":["print(f\"Baseline Model evaluation:\")\n","avg_length, distinct1, distinct2 = evaluate_diversity(baseline_model)\n","print(f\"Greedy decoding:\")\n","print(f\"Avg Response Length = {avg_length}\")\n","print(f\"Distinct1 = {distinct1}\")\n","print(f\"Distinct2 = {distinct2}\")\n","avg_length, distinct1, distinct2 = evaluate_diversity(baseline_model, mode=\"beam\")\n","print(f\"Beam decoding:\")\n","print(f\"Avg Response Length = {avg_length}\")\n","print(f\"Distinct1 = {distinct1}\")\n","print(f\"Distinct2 = {distinct2}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Baseline Model evaluation:\n","[['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'not', '!', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'good', 'night', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'two', 'years', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'right', 'back', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'fine', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'the', 'other', 'one', '.', '</s>'], ['<s>', 'i', 'm', 'not', 'going', 'to', 'the', 'hospital', '.', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'it', 's', 'a', 'surprise', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'going', 'to', 'the', 'car', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'you', 're', 'a', 'liar', '.', '</s>'], ['<s>', 'i', 'know', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'know', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'he', 's', 'dead', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'she', 's', 'not', 'here', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'll', 'be', 'right', 'back', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'yeah', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'it', 's', 'not', 'a', 'long', 'time', 'ago', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'know', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'good', 'night', '.', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'll', 'be', 'right', 'back', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'right', 'back', '.', '</s>'], ['<s>', 'you', 're', 'not', 'going', 'to', 'be', 'here', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'you', 're', 'not', 'sure', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'sure', '.', '</s>'], ['<s>', 'it', 's', 'not', 'a', 'long', 'time', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', 'going', 'to', 'the', 'car', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'no', '.', '</s>']]\n","Greedy decoding:\n","Avg Response Length = 3.58\n","Distinct1 = 0.12569832402234637\n","Distinct2 = 0.24806201550387597\n","[['<s>', 'i', 'm', '.', '</s>'], ['<s>', 'it', 's', 'not', '.', '</s>'], ['<s>', 'so', 'what', 'are', '.', '</s>'], ['<s>', 'i', 'm', '.', '.', '</s>'], ['<s>', 'i', 'm', 'here', '?', '</s>'], ['<s>', 'it', 's', 'that', '?', '</s>'], ['<s>', 'that', 's', 'not', 'a', 'good', '.', '</s>'], ['<s>', 'i', 'don', 't', 'it', '.', '</s>'], ['<s>', 'thank', 'you', 'can', 'i', '.', '</s>'], ['<s>', '.', '.', '</s>'], ['<s>', 'i', 'm', 'fine', '.', '</s>'], ['<s>', 'i', 'don', 't', 'it', '.', '</s>'], ['<s>', 'i', 'don', 't', 'he', 's', '?', '</s>'], ['<s>', 'what', 'do', '.', '</s>'], ['<s>', 'that', 's', 'not', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'here', 'for', '?', '</s>'], ['<s>', 'no', '!', '</s>'], ['<s>', 'it', 's', 'not', 'so', '.', '</s>'], ['<s>', 'i', 'm', 'here', '.', '</s>'], ['<s>', '.', '.', '</s>'], ['<s>', 'it', 's', '.', '</s>'], ['<s>', 'i', 'm', 'sure', '</s>'], ['<s>', 'i', 'don', 't', 'it', '.', '</s>'], ['<s>', 'sure', 'am', 'i', 'am', '.', '</s>'], ['<s>', 'i', 'm', '.', '.', '</s>'], ['<s>', 'i', 'll', 'i', 'do', 'that', '?', '</s>'], ['<s>', 'i', 'm', 'here', '.', '</s>'], ['<s>', 'oh', '.', '.', '</s>'], ['<s>', 'it', 's', 'not', 'what', 'i', 'm', '?', '</s>'], ['<s>', 'it', 's', 'not', 'here', '?', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>'], ['<s>', 'it', 's', 'guy', 'of', 'the', 'other', 'time', '</s>'], ['<s>', 'i', 'm', 'here', '?', '</s>'], ['<s>', 'it', 's', 'wrong', '?', '</s>'], ['<s>', 'i', 'm', 'sorry', '?', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>'], ['<s>', 'what', 'do', '.', '</s>'], ['<s>', 'i', 'don', 't', 'i', '?', '</s>'], ['<s>', 'what', 'do', '.', '</s>'], ['<s>', 'i', 'm', '?', '</s>'], ['<s>', 'it', 's', 'the', 'matter', '.', '</s>'], ['<s>', 'you', 're', 'not', '!', '</s>'], ['<s>', 'it', 's', 'not', 'me', '.', '</s>'], ['<s>', 'that', 's', 'a', 'lot', '?', '</s>'], ['<s>', 'that', 's', 'not', 'so', '.', '</s>'], ['<s>', 'i', 'm', '?', '</s>'], ['<s>', 'i', 'm', 'sieu', '?', '</s>'], ['<s>', 'i', 'm', '?', '</s>'], ['<s>', 'what', 'do', '.', '</s>'], ['<s>', 'yes', 'i', 'do', '?', '</s>'], ['<s>', 'what', 'do', '.', '</s>'], ['<s>', 'you', 're', 'not', '!', '</s>'], ['<s>', 'it', 'was', 'it', 's', '.', '</s>'], ['<s>', 'it', 's', 'wrong', '.', '.', '</s>'], ['<s>', 'i', 'm', 'sieu', '?', '</s>'], ['<s>', 'no', '!', '</s>'], ['<s>', 'it', 's', '.', '</s>'], ['<s>', 'it', 's', '.', '</s>'], ['<s>', 'i', 'don', 't', 'he', 's', 'in', '.', '.', '</s>'], ['<s>', 'it', 's', 'not', 'at', 'all', 'right', '</s>'], ['<s>', 'it', 's', 'not', 'here', '.', '</s>'], ['<s>', 'it', 's', 'not', 'really', '.', '</s>'], ['<s>', 'i', 'm', '!', '</s>'], ['<s>', 'yeah', '.', '.', '</s>'], ['<s>', 'you', 're', '!', '</s>'], ['<s>', 'i', 'm', '!', '</s>'], ['<s>', 'oh', '.', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'so', '.', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>'], ['<s>', 'no', '!', '</s>'], ['<s>', 'it', 's', 'wrong', '.', '</s>'], ['<s>', 'you', 're', 'you', 're', 'a', 'good', '.', '</s>'], ['<s>', 'i', 'm', '!', '</s>'], ['<s>', 'i', 'm', 'here', '.', '</s>'], ['<s>', 'oh', '.', '</s>'], ['<s>', 'it', 's', 'that', 'way', '?', '</s>'], ['<s>', 'i', 'm', '!', '</s>'], ['<s>', 'that', 's', 'a', 'man', '?', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'i', 'don', 't', 'you', '.', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'i', 'm', 'here', '.', '</s>'], ['<s>', 'oh', '.', '</s>'], ['<s>', 'i', 'don', 't', 'i', '?', '</s>'], ['<s>', 'thank', 'you', 'll', 'be', 'good', 'for', 'the', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'where', 'are', 'you', 're', 'not', '?', '</s>'], ['<s>', 'i', 'll', 'i', 'don', 't', '?', '</s>'], ['<s>', 'i', 'll', 'be', '?', '</s>'], ['<s>', 'you', 're', 'you', 'too', '?', '</s>'], ['<s>', 'no', '.', '.', '.', '</s>'], ['<s>', 'how', 'did', 'he', '?', '</s>'], ['<s>', 'it', 's', 'not', '.', '</s>'], ['<s>', 'i', 'don', 't', 'a', 'year', '.', '</s>'], ['<s>', 'i', 'm', 'here', '?', '</s>'], ['<s>', 'we', 'll', 'go', '!', '</s>'], ['<s>', 'i', 'm', '?', '</s>'], ['<s>', 'i', 'm', 'sieu', 'you', '!', '</s>'], ['<s>', 'i', 'm', '!', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>']]\n","Beam decoding:\n","Avg Response Length = 4.13\n","Distinct1 = 0.14285714285714285\n","Distinct2 = 0.4057507987220447\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ac_hU2STf_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618944099803,"user_tz":240,"elapsed":5130,"user":{"displayName":"Collin Avidano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtSMsv_vYn1lQSuqeCzg240kNa5YWmyrqeoYcBWy4=s64","userId":"02884380186597284052"}},"outputId":"484f653a-dffa-4e83-afca-1d10715a9fc8"},"source":["print(f\"Attention Model evaluation:\")\n","avg_length, distinct1, distinct2 = evaluate_diversity(attention_model)\n","print(f\"Greedy decoding:\")\n","print(f\"Avg Response Length = {avg_length}\")\n","print(f\"Distinct1 = {distinct1}\")\n","print(f\"Distinct2 = {distinct2}\")\n","avg_length, distinct1, distinct2 = evaluate_diversity(attention_model, mode=\"beam\")\n","print(f\"Beam decoding:\")\n","print(f\"Avg Response Length = {avg_length}\")\n","print(f\"Distinct1 = {distinct1}\")\n","print(f\"Distinct2 = {distinct2}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Attention Model evaluation:\n","[['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'right', 'back', '.', '</s>'], ['<s>', 'goodbye', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'yeah', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'that', 's', 'right', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'good', 'night', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'because', 'i', 'said', 'i', 'was', 'just', 'a', 'bit', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'oh', 'yeah', '?', '</s>'], ['<s>', 'i', 'll', 'be', 'there', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'what', 's', 'wrong', '?', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'm', 'fine', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'm', 'superstitious', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'there', '.', '</s>'], ['<s>', 'i', 'said', 'i', 'was', 'just', 'a', 'movie', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'four', 'million', 'dollars', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'it', 's', 'a', 'lovely', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'not', '.', '</s>'], ['<s>', 'i', 'said', 'i', 'was', 'pregnant', '.', '</s>'], ['<s>', 'i', 'm', 'fine', '.', '</s>'], ['<s>', 'i', 'said', 'i', 'said', '.', '.', '.', '</s>'], ['<s>', 'he', 's', 'in', 'the', 'bedroom', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'she', 's', 'a', 'narrow', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'thanks', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'oh', 'yeah', '?', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'i', 'don', 't', 'want', 'to', 'go', 'to', 'sleep', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'there', '.', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'i', 'm', 'not', 'gonna', 'make', 'it', '!', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'good', 'night', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'they', 're', 'not', 'gonna', 'kill', 'him', '!', '</s>'], ['<s>', 'you', 're', 'not', 'kidding', '.', '</s>'], ['<s>', 'you', 've', 'got', 'a', 'lot', 'of', 'thinking', '.', '</s>'], ['<s>', 'i', 'am', '.', '</s>'], ['<s>', 'you', 're', 'a', 'liar', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'four', 'days', '.', '</s>'], ['<s>', 'you', 're', 'not', 'gonna', 'make', 'it', 'all', 'right', '?', '</s>'], ['<s>', 'i', 'll', 'go', 'with', 'you', '.', '</s>'], ['<s>', 'i', 'll', 'be', 'there', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'i', 'don', 't', 'know', '.', '</s>'], ['<s>', 'no', '.', '</s>']]\n","Greedy decoding:\n","Avg Response Length = 4.19\n","Distinct1 = 0.16706443914081145\n","Distinct2 = 0.329153605015674\n","[['<s>', 'yes', 'i', 'didn', 't', '.', '</s>'], ['<s>', 'what', 'do', 'you', 're', 'a', 'liar', '?', '</s>'], ['<s>', 'how', 'much', '.', '.', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'you', 're', 'not', '.', '</s>'], ['<s>', 'what', 'do', 'you', 're', 'a', 'fool', '.', '</s>'], ['<s>', 'that', 's', 'great', '?', 'wow', '.', '</s>'], ['<s>', 'i', 'don', 't', 'it', '?', '</s>'], ['<s>', 'great', 'good', 'night', '?', '</s>'], ['<s>', 'yeah', '.', '</s>'], ['<s>', 'i', 'don', 't', 'matter', '.', '</s>'], ['<s>', 'it', 's', 'got', 'more', '.', '.', '.don', '</s>'], ['<s>', 'what', 'do', 'you', 're', 'a', 'stalker', '.', '</s>'], ['<s>', 'i', 'don', 't', 'talking', 'to', 'me', '!', '</s>'], ['<s>', 'you', 're', 'you', 're', 'a', 'cop', 'to', '?', '</s>'], ['<s>', 'thank', 'you', 'didn', 't', 'told', 'me', 'to', 'me', '</s>'], ['<s>', 'that', 's', 'got', 'it', '!', '</s>'], ['<s>', 'oh', '.', '.', '.', '</s>'], ['<s>', 'oh', 'yeah', '?', '</s>'], ['<s>', 'oh', 'yes', '?', '</s>'], ['<s>', 'it', 's', 'got', 'it', 's', 'rep', '</s>'], ['<s>', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'what', 's', 'wrong', '.', '</s>'], ['<s>', 'yes', 'i', 'am', 'i', 'm', 'sorry', '.', '</s>'], ['<s>', 'i', 'm', 'fine', '.', '</s>'], ['<s>', 'there', 'is', 'it', 'was', 'it', 'was', '?', '</s>'], ['<s>', 'oh', 'yeah', '?', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>'], ['<s>', 'what', 'do', 'you', 're', 'my', 'hair', '</s>'], ['<s>', 'you', 'did', '.', '.', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>'], ['<s>', 'i', 'm', 'you', 'talking', 'about', 'that', 's', 'too', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'that', 's', 'just', 'a', 'movie', '.', '</s>'], ['<s>', 'i', 'm', 'sorry', '?', '</s>'], ['<s>', 'i', 'said', 'yes', '.', '.', '</s>'], ['<s>', 'i', 'don', 't', 'talking', 'to', 'me', '!', '</s>'], ['<s>', 'yes', 'yes', 'yes', 'yes', '.', '</s>'], ['<s>', 'i', 'don', 't', 'talking', 'to', 'me', '!', '</s>'], ['<s>', 'i', 'don', 't', 'remember', '.', '</s>'], ['<s>', 'i', 'don', 't', 'saying', 'you', '.', '.', '</s>'], ['<s>', 'i', 'don', 't', 'be', '.', '</s>'], ['<s>', 'six', 'five', 'thousand', 'of', 'course', 'i', 'm', 'fine', '</s>'], ['<s>', 'that', 's', 'a', 'bit', 'me', 'too', '.', '</s>'], ['<s>', 'why', 'should', 'i', 'held', 'to', 'me', '.', '</s>'], ['<s>', 'i', 'm', 'fine', '.', '</s>'], ['<s>', 'oh', 'yeah', '.', '</s>'], ['<s>', 'i', 'm', '!', '</s>'], ['<s>', 'i', 'don', 't', 'talking', 'to', 'me', '!', '</s>'], ['<s>', 'yeah', 'i', 'don', 't', 'i', 't', '</s>'], ['<s>', 'i', 'don', 't', 'talking', 'to', 'me', '!', '</s>'], ['<s>', 'shut', 'up', '!', '!', 'tower', '</s>'], ['<s>', 'it', 'was', 'it', '.', '</s>'], ['<s>', 'you', 're', 'you', 're', 'a', 'lot', 'of', 'that', '</s>'], ['<s>', 'that', 's', 'not', '.', '</s>'], ['<s>', 'you', 're', 'your', 'name', '.', '</s>'], ['<s>', 'it', 's', 'got', 'a', 'minute', '.', '</s>'], ['<s>', 'that', 's', 'got', 'a', 'gun', '?', '</s>'], ['<s>', 'he', 's', 'in', 'the', 'answer', '.', '</s>'], ['<s>', 'i', 'don', 't', '?', '</s>'], ['<s>', 'that', 's', 'in', 'the', 'blond', 'that', 's', 'why', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>'], ['<s>', 'it', 's', 'that', 's', 'right', '.', '</s>'], ['<s>', 'thank', 'you', 'want', 'to', 'my', 'partner', '!', '</s>'], ['<s>', 'come', 'on', 'it', 's', '.', '</s>'], ['<s>', 'i', 'm', 'sieu', '?', '</s>'], ['<s>', 'i', 'm', 'sieu', 'u', '.', '</s>'], ['<s>', 'what', 'are', 'you', 're', 'you', '.', '</s>'], ['<s>', 'yeah', 'i', 'did', 'you', 'did', '?', '</s>'], ['<s>', 'i', 'don', 't', '?', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'yes', '.', '.', '.don', '.', '</s>'], ['<s>', 'it', 's', 'got', 'it', '?', '</s>'], ['<s>', 'that', 's', 'on', 'it', '?', '</s>'], ['<s>', 'yes', 'sir', '.', '.', '</s>'], ['<s>', 'right', 'now', '.', '.', '</s>'], ['<s>', 'it', 's', 'got', 'it', '?', '</s>'], ['<s>', 'yes', 'it', 'is', '?', '</s>'], ['<s>', 'yes', 'i', 'do', 'i', '</s>'], ['<s>', 'i', 'm', 'yes', '.', '</s>'], ['<s>', 'i', 'don', 't', '.', '</s>'], ['<s>', 'not', 'yet', '.', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'what', 'do', 'you', '?', '</s>'], ['<s>', 'what', '?', '</s>'], ['<s>', 'i', 'don', 't', 'be', '.', '</s>'], ['<s>', 'that', 's', 'going', 'to', 'talk', 'to', 'do', 'it', '?', '</s>'], ['<s>', 'i', 'm', 'sorry', 'of', 'your', 'wife', '!', '</s>'], ['<s>', 'you', 're', 'you', 'too', '?', '</s>'], ['<s>', 'then', 'what', 'are', 'you', 're', 'you', 'know', '?', '</s>'], ['<s>', 'yes', '.', '</s>'], ['<s>', 'then', 'what', 'are', 'you', 'did', '.', '</s>'], ['<s>', 'i', 'don', 't', 'you', 'know', '.', '</s>'], ['<s>', 'five', 'minutes', 'a', 'lot', '.', '.', '</s>'], ['<s>', 'yes', 'sir', '.', '</s>'], ['<s>', 'we', 'll', 'be', '!', '</s>'], ['<s>', 'i', 'm', '?', '</s>'], ['<s>', 'you', 're', 'you', 've', 'got', 'to', 'be', 'rich', '</s>'], ['<s>', 'no', '.', '</s>'], ['<s>', 'not', 'at', '.', '</s>']]\n","Beam decoding:\n","Avg Response Length = 5.06\n","Distinct1 = 0.20355731225296442\n","Distinct2 = 0.5246305418719212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xBXn697mBKDl"},"source":["## What to turn in?\n","\n","When you are done, make sure to run all the cells in your solution (including your conversation with the chatbot), and submit your notebook `Assignment 3 Final Release Version.ipynb` to Gradescope.\n"]}]}