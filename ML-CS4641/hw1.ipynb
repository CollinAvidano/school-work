{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FFy6leD3ZmzS"},"source":["# Fall 2019 CX4641/CS7641 Homework 1\n","\n","## Instructor: Dr. Mahdi Roozbahani\n","\n","## Deadline: Sep 12, Thursday, 11:59 pm\n","\n","* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n","\n","* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators."]},{"cell_type":"markdown","metadata":{"id":"C2Dz3TFIZmzT"},"source":["## Instructions for the assignment\n","\n","In this assignment, we only have writing questions: you are asked to answer them in the markdown cells.\n","\n","- Graduate students are required to complete all the questions including **bouns parts**. Undergraduate students are welcome to try bouns questions and we will add them on your final grade.\n","\n","- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n","    \n","- You could directly type the Latex equations in the markdown cell.\n","\n","- Typing with Latex is highly recommended. An image scan copy of handwritten also works. If you hand write, try to be clear as much as possible. No credit may be given to unreadable handwriting.\n","    \n","- If you want to add any picture to your answer, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook."]},{"cell_type":"markdown","metadata":{"id":"vurwpE07ZmzU"},"source":["## 1 Linear Algebra (25pts + 8pts)"]},{"cell_type":"markdown","metadata":{"id":"x6pSyvDOZmzU"},"source":["#### 1.1 Determinant and Inverse of Matrix [11pts]\n","Given a matrix M:\n","\n","$$M = \\begin{bmatrix} \n","  5 & 0 & 1 \\\\ \n","  6 & 1 & 2 \\\\\n","  0 & 4 & 3\n","  \\end{bmatrix}\n","$$\n","\n","- Calculate the determinant of M. [5pts]\n","(Calculation process required)\n","\n","$$5(1\\cdot3 - 2\\cdot4) - 0(6\\cdot3 - 2\\cdot0) + 1(6\\cdot4 - 1\\cdot0)$$\n","$$5(3 - 8) - 0(18 - 0) - 1(24 - 0)$$\n","$$-25 - 0 + 24$$\n","$$det(M) = -1$$\n","\n","- Does the inverse of M exist? If so, calculate $M^{-1}$. [6pts]\n","(Calculation process required)\n","\n","(**Hint:** please double check your answer and make sure $M M^{-1} = I$)\n","\n","$$\\begin{bmatrix}\n","5 & 0 & 1 \\\\\n","6 & 1 & 2 \\\\\n","0 & 4 & 3\n","\\end{bmatrix}\n","$$\n","\n","$$ Swap R_1 and R_2 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 2 \\\\\n","5 & 0 & 1 \\\\\n","0 & 4 & 3\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","0 & 1 & 0 \\\\\n","1 & 0 & 0 \\\\\n","0 & 0 & 1\n","\\end{bmatrix}\n","$$\n","\n","$$ R_2 = R_2 - \\frac{5}{6} \\cdot R_1 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 2 \\\\\n","0 & -\\frac{5}{6} & -\\frac{2}{3} \\\\\n","0 & 4 & 3\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","0 & 1 & 0 \\\\\n","1 & -\\frac{5}{6} & 0 \\\\\n","0 & 0 & 1\n","\\end{bmatrix}\n","$$\n","\n","$$ Swap R_2 and R_3 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 2 \\\\\n","0 & 4 & 3 \\\\\n","0 & -\\frac{5}{6} & -\\frac{2}{3} \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \\\\\n","1 & 0 & 0\n","\\end{bmatrix}\n","$$\n","\n","$$ R_3 = R_3 + \\frac{5}{24} \\cdot R_2 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 2 \\\\\n","0 & 4 & 3 \\\\\n","0 & 0 & -\\frac{1}{24} \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \\\\\n","1 & -\\frac{5}{6} & \\frac{5}{24}\n","\\end{bmatrix}\n","$$\n","\n","$$ R_3 = R_3 \\cdot -24 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 2 \\\\\n","0 & 4 & 3 \\\\\n","0 & 0 & 1 \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \\\\\n","-24 & 20 & -5\n","\\end{bmatrix}\n","$$\n","\n","$$ R_2 = R_2 - 3 \\cdot R_3 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 2 \\\\\n","0 & 4 & 0 \\\\\n","0 & 0 & 1 \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","0 & 1 & 0 \\\\\n","72 & -60 & 16 \\\\\n","-24 & 20 & -5\n","\\end{bmatrix}\n","$$\n","\n","$$ R_1 = R_1 - 2 \\cdot R_3 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 0 \\\\\n","0 & 4 & 0 \\\\\n","0 & 0 & 1 \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","48 & -39 & 10 \\\\\n","72 & -60 & 16 \\\\\n","-24 & 20 & -5\n","\\end{bmatrix}\n","$$\n","\n","$$ R_2 = \\frac{1}{4} \\cdot R_2 $$\n","$$\\begin{bmatrix}\n","6 & 1 & 0 \\\\\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","48 & -39 & 10 \\\\\n","18 & -15 & 4 \\\\\n","-24 & 20 & -5\n","\\end{bmatrix}\n","$$\n","\n","$$ R_1 = R_1 - 1 \\cdot R_2 $$\n","$$\\begin{bmatrix}\n","6 & 0 & 0 \\\\\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","30 & -24 & 6 \\\\\n","18 & -15 & 4 \\\\\n","-24 & 20 & -5\n","\\end{bmatrix}\n","$$\n","\n","$$ R_1 = \\frac{1}{6} \\cdot R_1 $$\n","$$\\begin{bmatrix}\n","1 & 0 & 0 \\\\\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","5 & -4 & 1 \\\\\n","18 & -15 & 4 \\\\\n","-24 & 20 & -5\n","\\end{bmatrix}\n","$$\n","\n","$$ M^{-1} = \n","\\begin{bmatrix}\n","5 & -4 & 1 \\\\\n","18 & -15 & 4 \\\\\n","-24 & 20 & -5\n","\\end{bmatrix}\n","$$\n","  \n","  \n","### 1.2 Characteristic Equation [8pts] (BONUS)\n","Consider the eigenvalue problem: \n","  $$Ax =\\lambda x, x \\neq 0$$\n","where $x$ is a non-zero eigenvector and $\\lambda$ is eigenvalue of $A$. Prove that the determinant $|A-\\lambda I|= 0$.\n","\n","(**Hint**: If a matrix is not full-rank (has linearly dependent columns), it is singular and non-invertible)\n","\n","\n","### 1.3 Eigenvalue [7pts]\n","Following 1.2, given a matrix $A$:\n","\n","$$A = \n","\\begin{bmatrix}\n","1 & r \\\\ \n","r & 1 \n","\\end{bmatrix}\n","$$\n","\n","$$(A - \\lambda I) =\n","\\begin{bmatrix} \n","1 & r \\\\ \n","r & 1 \n","\\end{bmatrix} - \n","\\begin{bmatrix} \n","\\lambda & 0 \\\\ \n","0 & \\lambda\n","\\end{bmatrix}\n","$$\n","\n","$$ = \n","\\begin{bmatrix} \n","\\lambda - 1 & r \\\\ \n","r & \\lambda - 1 \n","\\end{bmatrix}\n","$$\n","\n","$$ (\\lambda - 1)(\\lambda - 1) - (r)(r) $$\n","$$ = \\lambda^{2} - 2\\lambda + + 1  - r^{2} $$\n","$$ = -r^{2} + \\lambda^{2} - 2\\lambda + 1 $$\n","$$ \\lambda_1 = -r + 1 $$\n","$$ \\lambda_2 = r + 1 $$\n","\n","    \n","Calculate all the eigenvalues of $A$. (Calculation process required. Your answer should be expressed as a function of $r$.)\n","\n","### 1.4 Eigenvector [7pts]\n","Following 1.3, given that the $l_2$ norm of each eigenvector is 1, what are the eigenvectors of matrix $A$? For example, if an eigenvector is \n","    ${v}=\\begin{bmatrix} \n","    x1 \\\\ \n","    x2 \n","    \\end{bmatrix}$, then $||v||_2 = \\sqrt{x_1^2 + x_2^2} = 1$ (Calculation process required.)\n","\n","$$ (\\lambda I - A)x = 0 $$\n","$$ = \n","\\begin{bmatrix}\n","\\lambda - 1 & r \\\\ \n","r & \\lambda - 1 \n","\\end{bmatrix}\n","$$\n","\n","For $\\lambda_1$  \n","$$ = \\begin{bmatrix} \n","-r + 1 - 1 & r \\\\ \n","r & -r + 1 - 1 \n","\\end{bmatrix}\n","$$\n","\n","$$ = \\begin{bmatrix} \n","-r & r \\\\ \n","r & -r\n","\\end{bmatrix}\n","$$\n","\n","$$ = \\begin{bmatrix} \n","-r & r \\\\\n","0 & 0\n","\\end{bmatrix} = 0\n","$$\n","$$ x_1r = x_2r $$\n","\n","$$ (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}) $$\n","\n","\n","For $\\lambda_2$  \n","$$ = \\begin{bmatrix} \n","r + 1 - 1 & r \\\\ \n","r & r + 1 - 1 \n","\\end{bmatrix}\n","$$\n","\n","$$ = \\begin{bmatrix} \n","r & r \\\\ \n","r & r\n","\\end{bmatrix}\n","$$\n","\n","$$ = \\begin{bmatrix} \n","r & r \\\\ \n","0 & 0\n","\\end{bmatrix}\n","$$\n","\n","$$ x_1 = -x_2 $$\n","$$ = \\begin{bmatrix} \n","r \\\\ \n","-r\n","\\end{bmatrix} = 0\n","$$\n","\n","$$ (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}) $$\n","The negative of these are also eigen vectors\n","\n","\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"7D5eQGGiZmzV"},"source":["## 2 Expectation, Co-variance and Independence [25pts + 5pts]"]},{"cell_type":"markdown","metadata":{"id":"cMqPtRogZmzV"},"source":["Suppose $X, Y$ and $Z$ are three different random variables.\n","Let $X$ obeys Bernouli Distribution. The probability disbribution function is\n","    $$p(x)=\\left\\{\n","    \\begin{array}{c l}\t\n","         0.5 & x = c\\\\\n","         0.5 & x = -c.\n","    \\end{array}\\right.$$\n","    $c$ is a constant here.\n","    \n","    \n","- Let $Y$ obeys the standard Normal (Gaussian) distribution, which can be written as $Y \\sim N(0,1)$. $X$ and $Y$ are independent. Meanwhile, let $Z = XY$.\n","\n","- What is the Expectation and Variance of $X$? (in terms of $c$) [4pts]  \n","Since both values occur with equal probability and the only term affecting the expectation is then c both positive and negative we can just look for the midpoint intuitively which is\n","$$E[X] = 0$$\n","$$\\sigma^{2} = c^{2}$$\n","\n","- Show that when c=1, Z is a standard Normal(Gaussian) distribution, which means Zâˆ¼N(0,1)\n","Because of symmetry we can say that \n","P(X < z) = P(X > -z)\n","\n","P(Z < z) = P(XY < z) = P (XY < z | Y = 1)P(Y = 1) + P(XY < z| Y = -1)P(Y = -1) = \\frac{1}{2}P(X < z) + \\frac{1}{2}P(-X < z) = \\frac{1}{2}P(X < z) + \\frac{1}{2}P(X > -z) = \\frac{1}{2}P(X < z) + \\frac{1}{2}P(X < z) = P(X < z)\n","\n","- How should we choose $c$ such that Y and Z are uncorrelated (which means $Cov(Y,Z) = 0$)? [9pts]  \n","Cov(Y, Z) = E[YZ] - E[Y]E[Z]\n","\n","Calculate covariance in terms of c and then solve with covariance = 0\n","\n","$$ Cov(Y,Z) = \\frac{\\sum (Y_i - \\bar{Y})(Z_j - \\bar{Z})} {n} $$\n","Since a choice of zero for c would mean that the mean and variance of X are both 0.  \n","??? Can you claim this ???  \n","Since the mean of Z is zero and the samples taken from Z are now 0 as 0 * $Y_j$ = 0 then the covariance of the above is now  \n","$$ Cov(Y,Z) = \\frac{\\sum (Y_i - \\bar{Y})(0)} {n} $$  \n","Which will always be zero.\n","\n","\n","- Are Y and Z independent? (Just clarify) [3pts]  \n","\n","\n","- Show your conclusion for the above question with an example. **(Bouns)** [5pts]  \n"," P"]},{"cell_type":"markdown","metadata":{"id":"hHsbeDGNZmzX"},"source":["## 3 Maximum Likelihood [25pts + 10pts]"]},{"cell_type":"markdown","metadata":{"id":"QSEe9QB9ZmzY"},"source":["### 3.1 Discrete Example [15pts]\n","Suppose you are playing two unfair coins. The probability of tossing a head is $2 \\theta$ for coin 1, and $\\theta$ for coin 2. You toss each coin for several times, and you get the following results:\n","\n","| Coin No. | Result    |\n","|------|------|\n","|   1  | head |\n","|   2  | head |\n","|   1  | tail |\n","|   2  | tail |\n","|   1  | head |\n","|   2  | tail |\n","\n","- What is the probability of tossing a tail for coin 1 ($p_{t1}$) and tossing a tail for coin 2 ($p_{t2}$) [3pts]? \n","From the observed data we see that:\n","For coin 1: $ p_{t1} = 1 - (2 * \\theta) $\n","For coin 2: $ p_{t2} = 1 - \\theta $\n","\n","\n","- What is the likelihood of the data given $\\theta$ [6pts]?\n","$$P(X|\\theta) = 2\\theta * \\theta * (1 - 2\\theta) * (1 - \\theta) * 2\\theta * (1 - \\theta) $$\n","$$ = 4(1-2\\theta)\\theta^{3}(1-\\theta)^{2} $$\n","\n","\n","- What is maximum likelihood estimation for $\\theta$ [6pts]?\n","$$ Max of 4(1-2\\theta)\\theta^{3}(1-\\theta)^{2} -> $$\n","$$ \\frac{d}{d\\theta} 4(1-2\\theta)\\theta^{3}(1-\\theta)^{2} = $$\n","$$ 4(-12\\theta^{5} + 25\\theta^{4} - 16\\theta^{3} + 3\\theta^{2}) $$\n","$$ 4(-12\\theta^{5} + 25\\theta^{4} - 16\\theta^{3} + 3\\theta^{2}) = 0 $$\n","$$\\theta has roots at 0, 1, \\frac{1}{3}, \\frac{3}{4}$$\n","$$\\theta = \\frac{1}{3}$$\n","\n","\n","\n","### 3.2 Continues Example [10pts] (BONUS)\n","\n","A uniform distribution in the range of $[a, b]$ is given by\n","\n","$$\n","f(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{b-a}} & {a \\leq x \\leq b} \\\\ {0} & {\\text { otherwise }}\\end{array}\\right.\n","$$\n","What is maximum likelihood estimation for $a$ and $b$?\n","(You need to show the derivation of your answer.)\n","\n","( **Hint**: Think of two cases, where $x < max(x_1, x_2, ..., x_n)$ and $x \\ge max(x_1, x_2, ..., x_n).)$\n","\n","### 3.3 Maximum A Posteriori (MAP) [10pts]\n","Suppose there exists an unknown parameter $\\theta$ that describe whether the sun will explode tomorrow. $\\theta = 1$ means the sun will explode and $\\theta = 0$ if it won't. The likelihood function is:\n","$$\n","P(yes|\\theta)=\\left\\{\\begin{array}{ll}{1/36} & {\\theta = 0} \\\\ {35/36} & {\\theta = 1}\\end{array}\\right.\n","$$\n","- What is the maximum likelihood estimate of $\\theta$?[3pts]\n","Yes = is the prediction of the machine is yes given the theta  \n","Yes | 1 is the probability the machine is right  \n","Yes | 0 is the probability the machine has lied  \n","$\\theta = 1$  \n","\n","- Maximum A Posteriori (MAP) estimator aims to maximize the value of $\\theta$ in $p(\\theta|yes)$. What is the MAP estimate of $\\theta$ given that $P(\\theta=0)\\gg P(\\theta=1)$? Comment on the result.[7pts]\n","\n","( **Hint**: You can use Bayes Rule to get $p(\\theta|yes)$ from the likelihood! )\n","\n","To know more about MAP, refer to wiki page:\n","https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"]},{"cell_type":"markdown","metadata":{"id":"Ym8gpKMHZmzY"},"source":["## 4 Information Theory [25pts + 7pts]"]},{"cell_type":"markdown","metadata":{"id":"3HSWti__DYx9"},"source":["### 4.1 Marginal Distribution [4pts]\n","Suppose the joint probability distribution of two binary random variables $X$ and $Y$ are given as follows.\n","$$\n","\\begin{array}{|c|c|c|}\\hline X | Y & {1} & {2} \\\\ \\hline 0 & {\\frac{1}{5}} & {\\frac{2}{5}} \\\\ \\hline 1 & {0} & \\frac{2}{5} \\\\ \\hline\\end{array}\n","$$\n","\n","- Show the marginal distribution of $X$ and $Y$, respectively. [4pts]\n","$$ P(X = 0) = P(X = 0, Y = 1) + P(X = 0, Y = 2) = \\frac{1}{5} + \\frac{2}{5} = \\frac{3}{5} $$ \n","$$ P(X = 1) = P(X = 1, Y = 1) + P(X = 1, Y = 2) = 0 + \\frac{2}{5} = \\frac{2}{5} $$ \n","$$ P(Y = 1) = P(X = 0, Y = 1) + P(X = 1, Y = 1) = \\frac{1}{5} + 0 = \\frac{1}{5} $$ \n","$$ P(Y = 2) = P(X = 0, Y = 2) + P(X = 1, Y = 2) = \\frac{2}{5} + \\frac{2}{5} = \\frac{4}{5} $$ \n","    \n","### 4.2 Mutual Information and Entropy [21pts]\n","Given a dataset as below.\n","$$\n","\\begin{array}{|c|c|c|c|c|c|}\\hline Day & Outlook & Temperature & Humidity & Wind & Play? \\\\ \\hline 1 & overcast & hot & normal & medium & yes \\\\ \\hline 2 & sunny & hot & high & weak & no \\\\ \\hline 3 & sunny & mild & normal & weak & yes \\\\ \\hline 4 & rain & cool & high & strong & no \\\\ \\hline 5 & overcast & cool & normal & strong & yes \\\\ \\hline 6 & rain & mild & normal & medium & no \\\\ \\hline 7 & sunny & mild & high & medium & yes\\\\ \\hline 8 & overcast & hot & normal & strong & no\\\\ \\hline 9 & rain & hot & high & weak & no\\\\ \\hline 10 & sunny & cool & normal & strong & yes\\\\\\hline\\end{array}\n","$$\n","\n","We want to decide whether to play or not to play basketball on a certain day. Each input has four features ($x_1$, $x_2$, $x_3$, $x_4$): Outlook, Temperature, Humidity, Wind. The decision (play vs no-play) is represented as $Y$.\n","\n","- Find entropy $H(Y)$. [4pts]\n","$H(Y) = - [(\\frac{5}{10})\\log_2 (\\frac{5}{10}) + (\\frac{5}{10})\\log_2 (\\frac{5}{10})] = 1$\n","  \n","  \n","- Find conditional entropy $H(Y|x_1)$, $H(Y|x_4)$, respectively. [8pts]  \n","  \n","P(o) = 0.3  \n","P(s) = 0.4  \n","P(r) = 0.3  \n","P(y,o) = 0.2  \n","P(y,s) = 0.3  \n","P(y,r) = 0  \n","P(n,o) = 0.1  \n","P(n,s) = 0.1  \n","P(n,r) = 0.3  \n","\n","$(Y | o) = 0.2log(\\frac{0.3}{0.2}) + 0.1log(\\frac{0.3}{0.1}) = 0.2748$  \n","$(Y | s) = 0.3log(\\frac{0.4}{0.3}) + 0.1log(\\frac{0.4}{0.1}) = 0.3245$  \n","$(Y | r) = 0log(\\frac{0.3}{0}) + 0.3log(\\frac{0.3}{0.3}) = 0$  \n","$= H(Y|x_1) = 0.6$\n","\n","P(w) = 0.3  \n","P(m) = 0.3  \n","P(s) = 0.4  \n","P(y,w) = 0.1  \n","P(y,m) = 0.2  \n","P(y,s) = 0.2  \n","P(n,w) = 0.2  \n","P(n,m) = 0.1  \n","P(n,s) = 0.2  \n","\n","$(Y | w) = 0.1log(\\frac{0.3}{0.1}) + 0.2log(\\frac{0.3}{0.2}) = 0.2755$  \n","$(Y | m) = 0.2log(\\frac{0.3}{0.2}) + 0.1log(\\frac{0.3}{0.1}) = 0.2755$  \n","$(Y | s) = 0.2log(\\frac{0.4}{0.2}) + 0.2log(\\frac{0.4}{0.2}) = 0.4$  \n","$= H(Y|x_2) = 0.951$\n","  \n","- Find mutual information $I(x_1, Y)$ and $I(x_4, Y)$ and determine whether which one ($x_1$ or $x_4$) is more informative. [5pts]\n","$I(X_i,Y) = H(Y) - H(Y|X_i)$  \n","\n","$I(x_1, Y) = 1 - 0.6 = 0.4$  \n","$I(x_4, Y) = 1 - 0.951 = 0.049$  \n","$I(x_1, Y) > I(x_4, Y)$ Therefore Outlook is more informative than Wind  \n","\n","- Find joint entropy $H(Y, x_3)$. [4pts]  \n","  \n","$ H(Y,X) = \\sum_{y,x}P(Y = y, X = x)\\cdot log(\\frac{1}{P(Y = y, X = x)})$   \n","\n","P(yes,normal) = 0.4  \n","P(yes,high) = 0.1  \n","P(no,normal) = 0.2  \n","P(no,high) = 0.3  \n","\n","$0.4log(\\frac{1}{0.4}) + 0.1log(\\frac{1}{0.1}) + 0.2log(\\frac{1}{0.2}) + 0.3log(\\frac{1}{0.3}) = 1.8464$\n","\n","  \n","### 4.3 Bonus Question [7pts]\n","- Suppose $X$ and $Y$ are independent. Show that $H(X|Y) = H(X)$. [2pts]\n","\n","\n","- Suppose $X$ and $Y$ are independent. Show that $H(X,Y) = H(X) + H(Y)$. [2pts]\n","\n","  \n","- Prove that the mutual information is symmetric, i.e., $I(X, Y) = I(Y, X)$ and $x_i \\in X, y_i \\in Y$ [3pts]\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"nKoA7WkamTpe"},"source":[""],"execution_count":null,"outputs":[]}]}